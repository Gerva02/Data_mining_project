colnames(res_2)<-c("metodo","hyperparameter","accuracy")
res<-as.tibble(res_2)
best.res_2<-rbind(res_2%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
res_2%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)))
best.res_2
cv<-5
p<-1-1/cv
n.train<-nrow(transformed.glass.train)
k.train <-ncol(transformed.glass.train)
labels_train <-transformed.glass.train$Type
knn.kmax<-10
tree.size.max<- length(pruned.tree.k)-2
mat_2<-as.data.frame(matrix(NA,nrow=(knn.kmax+tree.size.max),ncol=cv+2))
mat_2[1:knn.kmax,1]<-"knn"
mat_2[(knn.kmax+1):(knn.kmax+tree.size.max),1]<-"tree"
mat_2[1:knn.kmax,2]<-1:knn.kmax
mat_2[(knn.kmax+1):(knn.kmax+tree.size.max),2]<- pruned.tree.k[2:(length(pruned.tree.k)-1)]
set.seed(44)
group_num <- rep(c(1:cv), round(n.train/cv))
random_assign <- sample( c(1:cv),size = n.train - length(group_num)) # ne mancheranno sempre cv o meno quindi non ci serve replace
split<-sample( append(group_num, random_assign) , size=n.train) #non ha senso questo metodo, capisco avere stessa proporzione ma usiamo una tecnica più flessibile se modifichiamo cv o proporzione train/test all'inizio (30% nel test set mi sembra eccessivo)
#loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv
for (j in 1:cv){
out_norm_cv_2<-normalize_2(transformed.glass.train[split != j  ,-k.train] ,transformed.glass.train[split ==j ,-k.train])
train_cv<-out_norm_cv_2$train_norm
test_cv<-out_norm_cv_2$test_norm
#metodo knn
for (i in 1:knn.kmax){
knn.pred <- knn(train=train_cv, test=test_cv, cl=labels_train[split != j], k = i)
#knn.pred <- knnMCN(train=train_cv, test=test_cv, TstX = NULL, K = i, ShowObs = F)
#TENIAMO PRESENTE CHE FACENDO NORMALIZE IN QUESTO MODO IL     RUSLTATO CAMBIA TOTALMENTE E PASSA DA k=1 a k=4 CHE HA SENSO??? O FORSE CAMBIO DRASTICO...NORMALIZZARE COME AVEVO FATTO PRIMA       ERA SBAGLIATO E QUESTO MIGLIORAMENTO LO ATTESTA (CON E SENZA NORMALIZZAZIONE C'E UN CAMBIAMENTO BELLO STRONG). RAZIONALE CHE        USCISSE CON k=1 CHE QUINDI PROBABILMENTE ERA UNA E UNA SOLA VARIABILE DELL'UNICO PUNTO VICINO A FARE LA DIFFERENZA
mat_2[i,j+2]<-mean(knn.pred == labels_train[split ==j])
}
train.cv.tree <- transformed.glass.train[split != j,] #ho provato con quelli normalizzati ma non cambia nulla
test.cv.tree  <-transformed.glass.train[split == j,]
tree.glass.cv <- tree(Type ~. , train.cv.tree)
#metodo alberi
for (l in 1:tree.size.max){
pruned.tree <-prune.tree(tree.glass.cv ,method="misclass", k= pruned.tree.k[l+1] )
tree.pred <-predict(pruned.tree , test.cv.tree[,-7] , type = "class")
mat_2[knn.kmax+l,j+2]<- mean(tree.pred == labels_train[split ==j])
}
}
res_2<-cbind(mat_2[,c(1,2)],apply(mat_2[,3:j+2],MARGIN=1,mean)) #possiamo usare median se vogliamo (meglio di no..)
colnames(res_2)<-c("metodo","hyperparameter","accuracy")
res<-as.tibble(res_2)
best.res_2<-rbind(res_2%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
res_2%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)))
best.res_2
cv<-5
p<-1-1/cv
n.train<-nrow(transformed.glass.train)
k.train <-ncol(transformed.glass.train)
labels_train <-transformed.glass.train$Type
knn.kmax<-10
tree.size.max<- length(pruned.tree.k)-2
mat_2<-as.data.frame(matrix(NA,nrow=(knn.kmax+tree.size.max),ncol=cv+2))
mat_2[1:knn.kmax,1]<-"knn"
mat_2[(knn.kmax+1):(knn.kmax+tree.size.max),1]<-"tree"
mat_2[1:knn.kmax,2]<-1:knn.kmax
mat_2[(knn.kmax+1):(knn.kmax+tree.size.max),2]<- pruned.tree.k[2:(length(pruned.tree.k)-1)]
set.seed(47)
group_num <- rep(c(1:cv), round(n.train/cv))
random_assign <- sample( c(1:cv),size = n.train - length(group_num)) # ne mancheranno sempre cv o meno quindi non ci serve replace
split<-sample( append(group_num, random_assign) , size=n.train) #non ha senso questo metodo, capisco avere stessa proporzione ma usiamo una tecnica più flessibile se modifichiamo cv o proporzione train/test all'inizio (30% nel test set mi sembra eccessivo)
#loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv
for (j in 1:cv){
out_norm_cv_2<-normalize_2(transformed.glass.train[split != j  ,-k.train] ,transformed.glass.train[split ==j ,-k.train])
train_cv<-out_norm_cv_2$train_norm
test_cv<-out_norm_cv_2$test_norm
#metodo knn
for (i in 1:knn.kmax){
knn.pred <- knn(train=train_cv, test=test_cv, cl=labels_train[split != j], k = i)
#knn.pred <- knnMCN(train=train_cv, test=test_cv, TstX = NULL, K = i, ShowObs = F)
#TENIAMO PRESENTE CHE FACENDO NORMALIZE IN QUESTO MODO IL     RUSLTATO CAMBIA TOTALMENTE E PASSA DA k=1 a k=4 CHE HA SENSO??? O FORSE CAMBIO DRASTICO...NORMALIZZARE COME AVEVO FATTO PRIMA       ERA SBAGLIATO E QUESTO MIGLIORAMENTO LO ATTESTA (CON E SENZA NORMALIZZAZIONE C'E UN CAMBIAMENTO BELLO STRONG). RAZIONALE CHE        USCISSE CON k=1 CHE QUINDI PROBABILMENTE ERA UNA E UNA SOLA VARIABILE DELL'UNICO PUNTO VICINO A FARE LA DIFFERENZA
mat_2[i,j+2]<-mean(knn.pred == labels_train[split ==j])
}
train.cv.tree <- transformed.glass.train[split != j,] #ho provato con quelli normalizzati ma non cambia nulla
test.cv.tree  <-transformed.glass.train[split == j,]
tree.glass.cv <- tree(Type ~. , train.cv.tree)
#metodo alberi
for (l in 1:tree.size.max){
pruned.tree <-prune.tree(tree.glass.cv ,method="misclass", k= pruned.tree.k[l+1] )
tree.pred <-predict(pruned.tree , test.cv.tree[,-7] , type = "class")
mat_2[knn.kmax+l,j+2]<- mean(tree.pred == labels_train[split ==j])
}
}
res_2<-cbind(mat_2[,c(1,2)],apply(mat_2[,3:j+2],MARGIN=1,mean)) #possiamo usare median se vogliamo (meglio di no..)
colnames(res_2)<-c("metodo","hyperparameter","accuracy")
res<-as.tibble(res_2)
best.res_2<-rbind(res_2%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
res_2%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)))
best.res_2
cv<-5
p<-1-1/cv
n.train<-nrow(glass.train)
k.train <-ncol(glass.train)
labels_train <-glass.train$Type
knn.kmax<-10
tree.size.max<- length(pruned.tree.k)-2
mat<-as.data.frame(matrix(NA,nrow=(knn.kmax+tree.size.max),ncol=cv+2))
mat[1:knn.kmax,1]<-"knn"
mat[(knn.kmax+1):(knn.kmax+tree.size.max),1]<-"tree"
mat[1:knn.kmax,2]<-1:knn.kmax
mat[(knn.kmax+1):(knn.kmax+tree.size.max),2]<- pruned.tree.k[2:(length(pruned.tree.k)-1)]
set.seed(47)
group_num <- rep(c(1:cv), round(n.train/cv))
random_assign <- sample( c(1:cv),size = n.train - length(group_num)) # ne mancheranno sempre cv o meno quindi non ci serve replace
split<-sample( append(group_num, random_assign) , size=n.train) #non ha senso questo metodo, capisco avere stessa proporzione ma usiamo una tecnica più flessibile se modifichiamo cv o proporzione train/test all'inizio (30% nel test set mi sembra eccessivo)
#loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv
for (j in 1:cv){
out_norm_cv<-normalize_2(glass.train[split != j  ,-k.train] ,glass.train[split ==j ,-k.train])
train_cv<-out_norm_cv$train_norm
test_cv<-out_norm_cv$test_norm
#metodo knn
for (i in 1:knn.kmax){
knn.pred <- knn(train=train_cv, test=test_cv, cl=labels_train[split != j], k = i)
#knn.pred <- knnMCN(train=train_cv, test=test_cv, TstX = NULL, K = i, ShowObs = F)
#TENIAMO PRESENTE CHE FACENDO NORMALIZE IN QUESTO MODO IL     RUSLTATO CAMBIA TOTALMENTE E PASSA DA k=1 a k=4 CHE HA SENSO??? O FORSE CAMBIO DRASTICO...NORMALIZZARE COME AVEVO FATTO PRIMA       ERA SBAGLIATO E QUESTO MIGLIORAMENTO LO ATTESTA (CON E SENZA NORMALIZZAZIONE C'E UN CAMBIAMENTO BELLO STRONG). RAZIONALE CHE        USCISSE CON k=1 CHE QUINDI PROBABILMENTE ERA UNA E UNA SOLA VARIABILE DELL'UNICO PUNTO VICINO A FARE LA DIFFERENZA
mat[i,j+2]<-mean(knn.pred == labels_train[split ==j])
}
train.cv.tree <- glass.train[split != j,]
test.cv.tree  <-glass.train[split ==j,]
tree.glass.cv <- tree(Type ~. , train.cv.tree)
#metodo alberi
for (l in 1:tree.size.max){
pruned.tree <-prune.tree(tree.glass.cv ,method="misclass", k= pruned.tree.k[l+1] )
tree.pred <-predict(pruned.tree , test.cv.tree[,-10] , type = "class")
mat[knn.kmax+l,j+2]<- mean(tree.pred == labels_train[split ==j])
}
}
res<-cbind(mat[,c(1,2)],apply(mat[,3:j+2],MARGIN=1,mean)) #possiamo usare median se vogliamo (meglio di no..)
colnames(res)<-c("metodo","hyperparameter","accuracy")
res<-as.tibble(res)
best.res<-rbind(res%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
res%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)))
best.res
out_norm<- normalize_2(transformed.glass.train[,-7],transformed.glass.test[,-7]) #k è lo stesso in ogni caso dai
train_norm<-out_norm$train_norm
test_norm<-out_norm$test_norm
set.seed(123)
knn.pred.transf <- knn(train=train_norm, test=test_norm, cl=labels_train, k = 3) #non funziona se ci escono 2 o più soluzioni
#TOCCA SCEGLIERE IL SEED E USIAMO QUEL CHE CI ESCE
mean(glass.test$Type == knn.pred.transf) #molto più alto rispetto al validation...temo che usare solo 214 record è poco...
out_norm<- normalize_2(transformed.glass.train[,-7],transformed.glass.test[,-7]) #k è lo stesso in ogni caso dai
train_norm<-out_norm$train_norm
test_norm<-out_norm$test_norm
set.seed(47)
knn.pred.transf <- knn(train=train_norm, test=test_norm, cl=labels_train, k = 3) #non funziona se ci escono 2 o più soluzioni
#TOCCA SCEGLIERE IL SEED E USIAMO QUEL CHE CI ESCE
mean(glass.test$Type == knn.pred.transf) #molto più alto rispetto al validation...temo che usare solo 214 record è poco...
#esiste anche knn.cv !!!!!!!!!
Kmax<-20
knn.pred.k<-rep(0,Kmax)
X_train<- out$train_norm
labels<-glass.train$Type #qua ho cambiato e non sembra essere cambiato nulla
X_test<- out$test_norm
for (i in 1:Kmax){
set.seed(47)
knn.pred <- knn(train=X_train, test=X_test, cl=labels, k = i)
knn.pred.k[i]<-mean(knn.pred == glass.test$Type)
}
r<-rep("black",Kmax)
r[which.max(knn.pred.k)]<-"red"
plot(1:Kmax,knn.pred.k,type="h",ylim=c(0,1),col=r)
abline(h=max(knn.pred.k),col="purple") #mi sa che ho rotto qualcosa ma è un po' incoprensibile che cosa
#esiste anche knn.cv !!!!!!!!!
Kmax<-20
set.seed(47)
knn.pred.k<-rep(0,Kmax)
X_train<- out$train_norm
labels<-glass.train$Type #qua ho cambiato e non sembra essere cambiato nulla
X_test<- out$test_norm
for (i in 1:Kmax){
knn.pred <- knn(train=X_train, test=X_test, cl=labels, k = i)
knn.pred.k[i]<-mean(knn.pred == glass.test$Type)
}
r<-rep("black",Kmax)
r[which.max(knn.pred.k)]<-"red"
plot(1:Kmax,knn.pred.k,type="h",ylim=c(0,1),col=r)
abline(h=max(knn.pred.k),col="purple") #mi sa che ho rotto qualcosa ma è un po' incoprensibile che cosa
#qua ho fatto un albero e sembra buono (migliorerà soltanto se normalizziam
set.seed(47)
tree.glass <- tree(Type ~. , glass.train)
summary(tree.glass)
tree.pred <- predict(tree.glass , glass.test[-10] , type = "class")
plot(tree.glass); text(tree.glass, , pretty = 0) # albero completo # queste due linee vanno eseguite insieme
mean(tree.pred == glass.test$Type)
pruned.tree.k <-prune.tree(tree.glass  ,method="misclass")$k
mean(tree.pred == glass.test$Type) #VEDERE DOVE INSERIRE QUESTO VALORE
#qua ho fatto un albero e sembra buono (migliorerà soltanto se normalizziam
set.seed(47)
tree.glass <- tree(Type ~. , glass.train)
summary(tree.glass)
tree.pred <- predict(tree.glass , glass.test[-10] , type = "class")
plot(tree.glass); text(tree.glass, , pretty = 0) # albero completo # queste due linee vanno eseguite insieme
mean(tree.pred == glass.test$Type)
pruned.tree.k <-prune.tree(tree.glass  ,method="misclass")$k
mean(tree.pred == glass.test$Type) #VEDERE DOVE INSERIRE QUESTO VALORE
knitr::opts_chunk$set(echo = T)
knitr::opts_chunk$set(comment = NA)
library(tidyverse)
library(mclust)
library(Rmixmod)
library(GGally)
library(caret)
library("tree")
library(ggcorrplot)
library(class)
library(nnet)
library(car)
#install.packages("hunspell")
path <- "glass.csv"
glass <- tibble(read.table(path , sep = "," , header = T))
glass$Type <- factor(glass$Type)
n<-nrow(glass)
k<-ncol(glass)
#questa è una funzione che divide tra test e train (magari ci serve nel CV)
dataset_division <- function(D1, perc = 0.8) {
set.seed(42)
id <- sample(size = round(perc* nrow(D1)) , x = nrow(D1))
train <- D1[id,]
test <- D1[-id,]
out <- list(train = train, test = test)
}
#Qua abbiamo diviso magari dobbiamo normalizzare
glass.train <- dataset_division(glass)$train #non ha senso 30% nel test set se facciamo cv (????)
glass.test <- dataset_division(glass)$test
#non trovo il comando per farlo per tutto il data set senza usare apply (dovremmo fare un apply)
# ma inoltre per ogni colonna (se normalizziamo) dobbiamo salvare min e max (e dopo veranno applicati sul test)
# glass.train%>%
#   select(-Type) %>%
#   ggpairs(mapping = aes(color = glass.train$Type))
ggcorrplot(cor(glass.train%>%
select(-Type)), lab = T , type = "lower" ) +
labs(title = "Plot di Correlazioni Lineari") #FAREI I GRAFICI SU "Glass" intero....(e metterei la divisione train/test dopo tutti i grafici)
#questo bar plot è riguardo freq relative
glass.train %>%
ggplot(aes(x= Type, y = after_stat(count)/sum(after_stat(count)), fill = Type)) +
geom_bar() +
labs(title = "Frequenza Relativa delle Classi",y= "Freq Rel")
#questo bar plot è riguardo freq assolute
glass.train %>%
ggplot(aes(x= Type, y = after_stat(count), fill = Type)) +
geom_bar() +
labs(title = "Frequenza Assoluta delle Classi",y= "Freq Ass")
glass.train %>%
select(-Type)%>%
gather(key = tipo, value=valore)%>%
ggplot(mapping= aes(y = valore)) +
geom_boxplot()+
facet_wrap(~ tipo,scales="free")+
labs(title = "Box Plot con Tutte le Variabili")
#qua devo ancora capire come farlo meglio sarebbero i box plot condizionati e ad ogni misurazione ad ogni tipo il problema è la scala di gradezza è diversa per ogni casella
glass.train %>%
gather(key = Measure, value = valore, -Type )  %>%
ggplot(aes(x = Type, y= valore, color = Measure)) +
geom_violin(trim = F)+
geom_boxplot(width=0.1) +
facet_wrap( ~ Measure ,scales = "free", ncol =3)+
labs(title = "Vilion plot condizionato alle classi")
#TODO boxplot confronto Ca e RI x la scelta di quella da togliere in knn
#Ma perchè metti To do qua quando c'è il read me (?), io ti consiglio di applicare malanobis
#che tiene conto di corr lineari
glass.train %>%
filter(Type == "6")
glass.train %>%
filter(if_any("RI" : "Fe", ~ . > 0 ))
glass.train %>%
gather(key = Measure, value = valore, -Type )  %>%
ggplot(aes(x= valore , color = Type)) +
geom_density() +
facet_wrap(~Measure, scales= "free")
p <-glass.train %>%
select( -c("Fe", "K" , "Ba", "Type"))
p.test <-glass.test %>%
select( -c("Fe", "K" , "Ba", "Type"))
p1 <- p %>%
apply(MARGIN =2 , FUN = powerTransform, family = "bcnPower")
a3 <- as.vector(c( p1$RI[1]$lambda,p1$Na[1]$lambda , p1$Mg[1]$lambda, p1$Al[1]$lambda, p1$Si[1]$lambda,p1$Ca[1]$lambda))
colnames(p)
col
transformedY <- bcPower(with(p, cbind(RI, Na, Mg, Al, Si, Ca)),
lambda = a3 ,gamma= 0.01)
transformedY.test <- bcPower(with(p.test, cbind(RI, Na, Mg, Al, Si, Ca)),
lambda = a3 ,gamma= 0.01)
transformed.glass.train <- tibble(cbind(as.data.frame(transformedY)), Type = glass.train$Type)
colnames(transformed.glass.train)<-c("RI_new","Na_new","Mg_new","Al_new","Si_new","Ca_new","Type")
transformed.glass.test <- tibble(cbind(as.data.frame(transformedY.test)), Type = glass.test$Type)
colnames(transformed.glass.test)<-c("RI_new","Na_new","Mg_new","Al_new","Si_new","Ca_new","Type")
transformed.glass.train%>%
gather(key = Measure, value = valore, -Type )  %>%
ggplot(aes(x= valore , colour= Type)) +
geom_density() +
facet_wrap(~Measure, scales= "free")
#qua si crea tabella pvalues
pvalue_shapiro <- matrix(0, nrow = (dim(glass.train)[2]-1), ncol = 6)
rownames(pvalue_shapiro) = colnames(glass.train)[-10]
colnames(pvalue_shapiro) = levels(glass.train$Type)
#-----
# Test Shapiro e costruzione di una matrice riassuntiva con i p-value condizionati alla classe
for (i in colnames(glass.train)[-10]){
for (j in levels(glass.train$Type)){
pvalue_shapiro[i,j]<-
shapiro.test( jitter(glass.train %>% filter(Type == j)%>%pull(i)) )$p.value #qua si assegnano tutti i valori, bisonga stare attenti perchè alcuni di questi non sono validi perchè ho messo jitter non lo fa andare in errore quando tutti i valori sono uguali ma introduce noise gaussiano che rende normali quando non sono ugugali
# pull invece serve per tirare fuori dal tibble un vettore (e non un tibble)
}
}
pvalue_shapiro["Ca", "6" ] <- pvalue_shapiro["K", "6" ] <- pvalue_shapiro["Fe", "6" ] <- NA
round(pvalue_shapiro, 5)
#qua si crea tabella pvalues
pvalue_shapiro_2 <- matrix(0, nrow = (dim(transformed.glass.train)[2]-1), ncol = 6)
rownames(pvalue_shapiro_2) = colnames(transformed.glass.train)[-7]
colnames(pvalue_shapiro_2) = levels(transformed.glass.train$Type)
#-----
# Test Shapiro e costruzione di una matrice riassuntiva con i p-value condizionati alla classe
for (i in colnames(transformed.glass.train)[-7]){
for (j in levels(glass.train$Type)){
pvalue_shapiro_2[i,j]<-
shapiro.test( jitter(transformed.glass.train %>% filter(Type == j)%>%pull(i)) )$p.value #qua si assegnano tutti i valori, bisonga stare attenti perchè alcuni di questi non sono validi perchè ho messo jitter non lo fa andare in errore quando tutti i valori sono uguali ma introduce noise gaussiano che rende normali quando non sono ugugali
# pull invece serve per tirare fuori dal tibble un vettore (e non un tibble)
}
}
#pvalue_shapiro["Ca", "6" ] <- pvalue_shapiro["K", "6" ] <- pvalue_shapiro["Fe", "6" ] <- NA
round(pvalue_shapiro_2, 5)
#v è un vettore colonna il primo valore è il minimo il secondo il massimo e il resto solo i valori effettivi delle colonne (Fe, Ba, etc)
#funzione che prende vettore, toglie il primo numero, e lo divide da secondo - primo
func<-function(v){
return((v-v[1])/(v[2]-v[1])) # non aggiorno v[1] v[2] fino alla fine e quelli saranno innutili
}
prova <-c(1,2,rnorm(100))
func(prova) #vediamo v[1] -> 0 v[2]->1
# matrix sarà la nostra tabella min e max
norm_aux<-function(data,matrix){
data_2<-rbind(matrix,data)
data_norm<-apply(data_2,MARGIN = 2,func) #applichiamo func ad ogni colonna func
return(data_norm[3:nrow(data_norm),]) # da 3 in poi perchè i primi due valori sono inutili
}
# questa funzione prende sia il train che il test e normalizza il test con min e max del train
normalize_2<-function(train,test){
matrix_min_max<-rbind(apply(train,MARGIN = 2,min),apply(train,MARGIN = 2,max)) #qua salviamo min e max -> matrice che anda in norm aux
train_norm<-norm_aux(train,matrix_min_max)
test_norm<-norm_aux(test,matrix_min_max)
return(list(train_norm=train_norm,test_norm=test_norm))
}
out<-normalize_2(glass.train[,-ncol(glass.train)],glass.test[,-ncol(glass.test)])
# out$train_norm[1:10,]
# out$test_norm[1:10,]
#mi viene da piangere, ha runnato tutto senza bug
#gervi fai un check che poi possiamo usarla
#qua ho fatto un albero e sembra buono (migliorerà soltanto se normalizziam
set.seed(47)
tree.glass <- tree(Type ~. , glass.train)
summary(tree.glass)
tree.pred <- predict(tree.glass , glass.test[-10] , type = "class")
plot(tree.glass); text(tree.glass, , pretty = 0) # albero completo # queste due linee vanno eseguite insieme
mean(tree.pred == glass.test$Type)
pruned.tree.k <-prune.tree(tree.glass  ,method="misclass")$k
mean(tree.pred == glass.test$Type) #VEDERE DOVE INSERIRE QUESTO VALORE
#esiste anche knn.cv !!!!!!!!!
Kmax<-20
set.seed(47)
knn.pred.k<-rep(0,Kmax)
X_train<- out$train_norm
labels<-glass.train$Type #qua ho cambiato e non sembra essere cambiato nulla
X_test<- out$test_norm
for (i in 1:Kmax){
knn.pred <- knn(train=X_train, test=X_test, cl=labels, k = i)
knn.pred.k[i]<-mean(knn.pred == glass.test$Type)
}
r<-rep("black",Kmax)
r[which.max(knn.pred.k)]<-"red"
plot(1:Kmax,knn.pred.k,type="h",ylim=c(0,1),col=r)
abline(h=max(knn.pred.k),col="purple") #mi sa che ho rotto qualcosa ma è un po' incoprensibile che cosa
#
# #si prova con mahalanobis distance
#
# mahalanobis_dist <- function(x, mean_vec, sigma_inv) {
#   diff_vec <- t(t(x) - mean_vec)
#   out <- sqrt(rowSums((diff_vec %*% sigma_inv) * diff_vec))
#   return(out)
# }
#
# find_mode <- function(x) {
#   u <- unique(x)
#   tab <- tabulate(match(x, u))
#   u[tab == max(tab)]
# }
#
# predict_label <- function(v, train, mat, k, train_lab) {
#   dists <- apply(train, MARGIN = 1, function(x) mahalanobis_dist(x, v, mat))
#   check <- data.frame("dist"=dists,"lab"=train_lab)
#   check <- as.tibble(check)
#   check_ord <- top_n(check, k, wt = dist)
#   return(find_mode(as.factor(check_ord$lab)))
# }
#
# # si implementa una funzione per l'applicazione del metodo knn tramite distanza di mahalanobis
# knn_mahalanobis <- function(train, valid, train_lab, valid_lab, k) {
#   x_matrix_inv <- solve(cor(train))
#   pred <- apply(valid, MARGIN = 1, function(x) predict_label(x, train, x_matrix_inv, k, train_lab))
#   out <- mean(as.factor(pred) == valid_lab)
#   return(out)
# }
#
#
#
# knn_mahalanobis(glass.train[,1:9], glass.test[,1:9], glass.train$Type, glass.test$Type, k=5) #NON VA
cv<-5
p<-1-1/cv
n.train<-nrow(glass.train)
k.train <-ncol(glass.train)
labels_train <-glass.train$Type
knn.kmax<-10
tree.size.max<- length(pruned.tree.k)-2
mat<-as.data.frame(matrix(NA,nrow=(knn.kmax+tree.size.max),ncol=cv+2))
mat[1:knn.kmax,1]<-"knn"
mat[(knn.kmax+1):(knn.kmax+tree.size.max),1]<-"tree"
mat[1:knn.kmax,2]<-1:knn.kmax
mat[(knn.kmax+1):(knn.kmax+tree.size.max),2]<- pruned.tree.k[2:(length(pruned.tree.k)-1)]
set.seed(47)
group_num <- rep(c(1:cv), round(n.train/cv))
random_assign <- sample( c(1:cv),size = n.train - length(group_num)) # ne mancheranno sempre cv o meno quindi non ci serve replace
split<-sample( append(group_num, random_assign) , size=n.train) #non ha senso questo metodo, capisco avere stessa proporzione ma usiamo una tecnica più flessibile se modifichiamo cv o proporzione train/test all'inizio (30% nel test set mi sembra eccessivo)
#loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv
for (j in 1:cv){
out_norm_cv<-normalize_2(glass.train[split != j  ,-k.train] ,glass.train[split ==j ,-k.train])
train_cv<-out_norm_cv$train_norm
test_cv<-out_norm_cv$test_norm
#metodo knn
for (i in 1:knn.kmax){
knn.pred <- knn(train=train_cv, test=test_cv, cl=labels_train[split != j], k = i)
#knn.pred <- knnMCN(train=train_cv, test=test_cv, TstX = NULL, K = i, ShowObs = F)
#TENIAMO PRESENTE CHE FACENDO NORMALIZE IN QUESTO MODO IL     RUSLTATO CAMBIA TOTALMENTE E PASSA DA k=1 a k=4 CHE HA SENSO??? O FORSE CAMBIO DRASTICO...NORMALIZZARE COME AVEVO FATTO PRIMA       ERA SBAGLIATO E QUESTO MIGLIORAMENTO LO ATTESTA (CON E SENZA NORMALIZZAZIONE C'E UN CAMBIAMENTO BELLO STRONG). RAZIONALE CHE        USCISSE CON k=1 CHE QUINDI PROBABILMENTE ERA UNA E UNA SOLA VARIABILE DELL'UNICO PUNTO VICINO A FARE LA DIFFERENZA
mat[i,j+2]<-mean(knn.pred == labels_train[split ==j])
}
train.cv.tree <- glass.train[split != j,]
test.cv.tree  <-glass.train[split ==j,]
tree.glass.cv <- tree(Type ~. , train.cv.tree)
#metodo alberi
for (l in 1:tree.size.max){
pruned.tree <-prune.tree(tree.glass.cv ,method="misclass", k= pruned.tree.k[l+1] )
tree.pred <-predict(pruned.tree , test.cv.tree[,-10] , type = "class")
mat[knn.kmax+l,j+2]<- mean(tree.pred == labels_train[split ==j])
}
}
res<-cbind(mat[,c(1,2)],apply(mat[,3:j+2],MARGIN=1,mean)) #possiamo usare median se vogliamo (meglio di no..)
colnames(res)<-c("metodo","hyperparameter","accuracy")
res<-as.tibble(res)
best.res<-rbind(res%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
res%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)))
best.res
res %>%
ggplot(mapping  = aes(x = hyperparameter, y= accuracy, color=metodo))  +
geom_line()+
geom_point()+
facet_grid(~metodo) +
labs(title= "Variazione di Accuracy Rispetto a Iperparametri")
cv<-5
p<-1-1/cv
n.train<-nrow(transformed.glass.train)
k.train <-ncol(transformed.glass.train)
labels_train <-transformed.glass.train$Type
knn.kmax<-10
tree.size.max<- length(pruned.tree.k)-2
mat_2<-as.data.frame(matrix(NA,nrow=(knn.kmax+tree.size.max),ncol=cv+2))
mat_2[1:knn.kmax,1]<-"knn"
mat_2[(knn.kmax+1):(knn.kmax+tree.size.max),1]<-"tree"
mat_2[1:knn.kmax,2]<-1:knn.kmax
mat_2[(knn.kmax+1):(knn.kmax+tree.size.max),2]<- pruned.tree.k[2:(length(pruned.tree.k)-1)]
set.seed(47)
group_num <- rep(c(1:cv), round(n.train/cv))
random_assign <- sample( c(1:cv),size = n.train - length(group_num)) # ne mancheranno sempre cv o meno quindi non ci serve replace
split<-sample( append(group_num, random_assign) , size=n.train) #non ha senso questo metodo, capisco avere stessa proporzione ma usiamo una tecnica più flessibile se modifichiamo cv o proporzione train/test all'inizio (30% nel test set mi sembra eccessivo)
#loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv
for (j in 1:cv){
out_norm_cv_2<-normalize_2(transformed.glass.train[split != j  ,-k.train] ,transformed.glass.train[split ==j ,-k.train])
train_cv<-out_norm_cv_2$train_norm
test_cv<-out_norm_cv_2$test_norm
#metodo knn
for (i in 1:knn.kmax){
knn.pred <- knn(train=train_cv, test=test_cv, cl=labels_train[split != j], k = i)
#knn.pred <- knnMCN(train=train_cv, test=test_cv, TstX = NULL, K = i, ShowObs = F)
#TENIAMO PRESENTE CHE FACENDO NORMALIZE IN QUESTO MODO IL     RUSLTATO CAMBIA TOTALMENTE E PASSA DA k=1 a k=4 CHE HA SENSO??? O FORSE CAMBIO DRASTICO...NORMALIZZARE COME AVEVO FATTO PRIMA       ERA SBAGLIATO E QUESTO MIGLIORAMENTO LO ATTESTA (CON E SENZA NORMALIZZAZIONE C'E UN CAMBIAMENTO BELLO STRONG). RAZIONALE CHE        USCISSE CON k=1 CHE QUINDI PROBABILMENTE ERA UNA E UNA SOLA VARIABILE DELL'UNICO PUNTO VICINO A FARE LA DIFFERENZA
mat_2[i,j+2]<-mean(knn.pred == labels_train[split ==j])
}
train.cv.tree <- transformed.glass.train[split != j,] #ho provato con quelli normalizzati ma non cambia nulla
test.cv.tree  <-transformed.glass.train[split == j,]
tree.glass.cv <- tree(Type ~. , train.cv.tree)
#metodo alberi
for (l in 1:tree.size.max){
pruned.tree <-prune.tree(tree.glass.cv ,method="misclass", k= pruned.tree.k[l+1] )
tree.pred <-predict(pruned.tree , test.cv.tree[,-7] , type = "class")
mat_2[knn.kmax+l,j+2]<- mean(tree.pred == labels_train[split ==j])
}
}
res_2<-cbind(mat_2[,c(1,2)],apply(mat_2[,3:j+2],MARGIN=1,mean)) #possiamo usare median se vogliamo (meglio di no..)
colnames(res_2)<-c("metodo","hyperparameter","accuracy")
res<-as.tibble(res_2)
best.res_2<-rbind(res_2%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
res_2%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)))
best.res_2
out_norm<- normalize_2(transformed.glass.train[,-7],transformed.glass.test[,-7]) #k è lo stesso in ogni caso dai
train_norm<-out_norm$train_norm
test_norm<-out_norm$test_norm
set.seed(47)
knn.pred.transf <- knn(train=train_norm, test=test_norm, cl=labels_train, k = 3) #non funziona se ci escono 2 o più soluzioni
#TOCCA SCEGLIERE IL SEED E USIAMO QUEL CHE CI ESCE
mean(glass.test$Type == knn.pred.transf) #molto più alto rispetto al validation...temo che usare solo 214 record è poco...
true<-glass.test$Type #VEDETE SE INSERIRE, PER ME CI STA
pred_top<-knn.pred.transf
confusionMatrix(pred_top,true)
