res.2<-as.tibble(res.2)
best.res.2<-rbind(res.2%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
res.2%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)))
best.res.2
best.res.2
cv<-5
p<-1-1/cv
n.train<-nrow(transformed.glass.train)
k.train <-ncol(transformed.glass.train)
labels.train <-transformed.glass.train$Type
knn.kmax<-10
tree.size.max<- length(pruned.tree.k)-2
knn.mahalanobis.max<-40
mat.2<-as.data.frame(matrix(NA,nrow=(knn.kmax+tree.size.max+knn.mahalanobis.max),ncol=cv+2))
mat.2[1:knn.kmax,1]<-"knn"
mat.2[(knn.kmax+1):(knn.kmax+tree.size.max),1]<-"tree"
mat.2[(knn.kmax+tree.size.max+1):(knn.kmax+tree.size.max+knn.mahalanobis.max),1]<-"knn.maha"
mat.2[1:knn.kmax,2]<-1:knn.kmax
mat.2[(knn.kmax+1):(knn.kmax+tree.size.max),2]<- pruned.tree.k[2:(length(pruned.tree.k)-1)]
mat.2[(knn.kmax+tree.size.max+1):(knn.kmax+tree.size.max+knn.mahalanobis.max),2]<- 1:knn.mahalanobis.max
set.seed(47)
group <- rep(c(1:cv), round(n.train/cv))
random.assign <- sample( c(1:cv),size = n.train - length(group)) # ne mancheranno sempre cv o meno quindi non ci serve replace
split<-sample( append(group, random.assign) , size=n.train) #non ha senso questo metodo, capisco avere stessa proporzione ma usiamo una tecnica più flessibile se modifichiamo cv o proporzione train/test all'inizio (30% nel test set mi sembra eccessivo)
#loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv
for (j in 1:cv){
out.norm.cv.2<-normalize(transformed.glass.train[split != j  ,-k.train] ,transformed.glass.train[split ==j ,-k.train])
train.cv<-out.norm.cv.2$train_norm
test.cv<-out.norm.cv.2$test_norm
#metodo knn
for (i in 1:knn.kmax){
knn.pred <- knn(train=train.cv, test=test.cv, cl=labels.train[split != j], k = i)
#knn.pred <- knnMCN(train=train_cv, test=test_cv, TstX = NULL, K = i, ShowObs = F)
#TENIAMO PRESENTE CHE FACENDO NORMALIZE IN QUESTO MODO IL     RUSLTATO CAMBIA TOTALMENTE E PASSA DA k=1 a k=4 CHE HA SENSO??? O FORSE CAMBIO DRASTICO...NORMALIZZARE COME AVEVO FATTO PRIMA       ERA SBAGLIATO E QUESTO MIGLIORAMENTO LO ATTESTA (CON E SENZA NORMALIZZAZIONE C'E UN CAMBIAMENTO BELLO STRONG). RAZIONALE CHE        USCISSE CON k=1 CHE QUINDI PROBABILMENTE ERA UNA E UNA SOLA VARIABILE DELL'UNICO PUNTO VICINO A FARE LA DIFFERENZA
mat.2[i,j+2]<-mean(knn.pred == labels.train[split ==j])
}
train.cv.tree <- transformed.glass.train[split != j,] #ho provato con quelli normalizzati ma non cambia nulla
test.cv.tree  <-transformed.glass.train[split == j,]
tree.glass.cv <- tree(Type ~. , train.cv.tree)
#metodo alberi
for (l in 1:tree.size.max){
pruned.tree <-prune.tree(tree.glass.cv ,method = "misclass", k = pruned.tree.k[l+1] )
tree.pred <-predict(pruned.tree , test.cv.tree[,-k.train] , type = "class")
mat.2[knn.kmax+l,j+2]<- mean(tree.pred == labels.train[split ==j])
}
train.maha <- transformed.glass.train[split != j,]
test.maha <-transformed.glass.train[split ==j,]
#metodo knn maha
for (h in 1:knn.mahalanobis.max){
mat.2[knn.kmax+tree.size.max+h,j+2]<- knn_mahalanobis(train.maha[,-k.train], test.maha[,-k.train], train.maha$Type, test.maha$Type, k=h)
}
}
res.2<-cbind(mat.2[,c(1,2)],apply(mat.2[,3:j+2],MARGIN=1,mean))
colnames(res.2)<-c("metodo","hyperparameter","accuracy")
res.2<-as.tibble(res.2)
best.res.2<-rbind(res.2%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
res.2%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)),
res.2%>%filter(metodo=="knn.maha")%>%filter(accuracy==max(accuracy)))
best.res.2
res.2 %>%
ggplot(mapping  = aes(x = hyperparameter, y= accuracy, color=metodo))  +
geom_line()+
geom_point()+
facet_grid(~metodo) +
labs(title= "Variazione di Accuracy Rispetto a Iperparametri") #DA CAMBIARE IL TITOLO....
cv<-5
p<-1-1/cv
n.train<-nrow(transformed.glass.train)
k.train <-ncol(transformed.glass.train)
labels.train <-transformed.glass.train$Type
knn.kmax<-10
tree.size.max<- length(pruned.tree.k)-2
knn.mahalanobis.max<-40
mat.2<-as.data.frame(matrix(NA,nrow=(knn.kmax+tree.size.max+knn.mahalanobis.max),ncol=cv+2))
mat.2[1:knn.kmax,1]<-"knn"
mat.2[(knn.kmax+1):(knn.kmax+tree.size.max),1]<-"tree"
mat.2[(knn.kmax+tree.size.max+1):(knn.kmax+tree.size.max+knn.mahalanobis.max),1]<-"knn.maha"
mat.2[1:knn.kmax,2]<-1:knn.kmax
mat.2[(knn.kmax+1):(knn.kmax+tree.size.max),2]<- pruned.tree.k[2:(length(pruned.tree.k)-1)]
mat.2[(knn.kmax+tree.size.max+1):(knn.kmax+tree.size.max+knn.mahalanobis.max),2]<- 1:knn.mahalanobis.max
group <- rep(c(1:cv), round(n.train/cv))
random.assign <- sample( c(1:cv),size = n.train - length(group)) # ne mancheranno sempre cv o meno quindi non ci serve replace
split<-sample( append(group, random.assign) , size=n.train) #non ha senso questo metodo, capisco avere stessa proporzione ma usiamo una tecnica più flessibile se modifichiamo cv o proporzione train/test all'inizio (30% nel test set mi sembra eccessivo)
#loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv
for (j in 1:cv){
out.norm.cv.2<-normalize(transformed.glass.train[split != j  ,-k.train] ,transformed.glass.train[split ==j ,-k.train])
train.cv<-out.norm.cv.2$train_norm
test.cv<-out.norm.cv.2$test_norm
#metodo knn
for (i in 1:knn.kmax){
knn.pred <- knn(train=train.cv, test=test.cv, cl=labels.train[split != j], k = i)
#knn.pred <- knnMCN(train=train_cv, test=test_cv, TstX = NULL, K = i, ShowObs = F)
#TENIAMO PRESENTE CHE FACENDO NORMALIZE IN QUESTO MODO IL     RUSLTATO CAMBIA TOTALMENTE E PASSA DA k=1 a k=4 CHE HA SENSO??? O FORSE CAMBIO DRASTICO...NORMALIZZARE COME AVEVO FATTO PRIMA       ERA SBAGLIATO E QUESTO MIGLIORAMENTO LO ATTESTA (CON E SENZA NORMALIZZAZIONE C'E UN CAMBIAMENTO BELLO STRONG). RAZIONALE CHE        USCISSE CON k=1 CHE QUINDI PROBABILMENTE ERA UNA E UNA SOLA VARIABILE DELL'UNICO PUNTO VICINO A FARE LA DIFFERENZA
mat.2[i,j+2]<-mean(knn.pred == labels.train[split ==j])
}
train.cv.tree <- transformed.glass.train[split != j,] #ho provato con quelli normalizzati ma non cambia nulla
test.cv.tree  <-transformed.glass.train[split == j,]
tree.glass.cv <- tree(Type ~. , train.cv.tree)
#metodo alberi
for (l in 1:tree.size.max){
pruned.tree <-prune.tree(tree.glass.cv ,method = "misclass", k = pruned.tree.k[l+1] )
tree.pred <-predict(pruned.tree , test.cv.tree[,-k.train] , type = "class")
mat.2[knn.kmax+l,j+2]<- mean(tree.pred == labels.train[split ==j])
}
train.maha <- transformed.glass.train[split != j,]
test.maha <-transformed.glass.train[split ==j,]
#metodo knn maha
for (h in 1:knn.mahalanobis.max){
mat.2[knn.kmax+tree.size.max+h,j+2]<- knn_mahalanobis(train.maha[,-k.train], test.maha[,-k.train], train.maha$Type, test.maha$Type, k=h)
}
}
res.2<-cbind(mat.2[,c(1,2)],apply(mat.2[,3:j+2],MARGIN=1,mean))
colnames(res.2)<-c("metodo","hyperparameter","accuracy")
res.2<-as.tibble(res.2)
best.res.2<-rbind(res.2%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
res.2%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)),
res.2%>%filter(metodo=="knn.maha")%>%filter(accuracy==max(accuracy)))
best.res.2
cv<-5
p<-1-1/cv
n.train<-nrow(transformed.glass.train)
k.train <-ncol(transformed.glass.train)
labels.train <-transformed.glass.train$Type
knn.kmax<-10
tree.size.max<- length(pruned.tree.k)-2
knn.mahalanobis.max<-40
mat.2<-as.data.frame(matrix(NA,nrow=(knn.kmax+tree.size.max+knn.mahalanobis.max),ncol=cv+2))
mat.2[1:knn.kmax,1]<-"knn"
mat.2[(knn.kmax+1):(knn.kmax+tree.size.max),1]<-"tree"
mat.2[(knn.kmax+tree.size.max+1):(knn.kmax+tree.size.max+knn.mahalanobis.max),1]<-"knn.maha"
mat.2[1:knn.kmax,2]<-1:knn.kmax
mat.2[(knn.kmax+1):(knn.kmax+tree.size.max),2]<- pruned.tree.k[2:(length(pruned.tree.k)-1)]
mat.2[(knn.kmax+tree.size.max+1):(knn.kmax+tree.size.max+knn.mahalanobis.max),2]<- 1:knn.mahalanobis.max
set.seed(47)
group <- rep(c(1:cv), round(n.train/cv))
random.assign <- sample( c(1:cv),size = n.train - length(group)) # ne mancheranno sempre cv o meno quindi non ci serve replace
split<-sample( append(group, random.assign) , size=n.train) #non ha senso questo metodo, capisco avere stessa proporzione ma usiamo una tecnica più flessibile se modifichiamo cv o proporzione train/test all'inizio (30% nel test set mi sembra eccessivo)
#loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv
for (j in 1:cv){
out.norm.cv.2<-normalize(transformed.glass.train[split != j  ,-k.train] ,transformed.glass.train[split ==j ,-k.train])
train.cv<-out.norm.cv.2$train_norm
test.cv<-out.norm.cv.2$test_norm
#metodo knn
for (i in 1:knn.kmax){
knn.pred <- knn(train=train.cv, test=test.cv, cl=labels.train[split != j], k = i)
#knn.pred <- knnMCN(train=train_cv, test=test_cv, TstX = NULL, K = i, ShowObs = F)
#TENIAMO PRESENTE CHE FACENDO NORMALIZE IN QUESTO MODO IL     RUSLTATO CAMBIA TOTALMENTE E PASSA DA k=1 a k=4 CHE HA SENSO??? O FORSE CAMBIO DRASTICO...NORMALIZZARE COME AVEVO FATTO PRIMA       ERA SBAGLIATO E QUESTO MIGLIORAMENTO LO ATTESTA (CON E SENZA NORMALIZZAZIONE C'E UN CAMBIAMENTO BELLO STRONG). RAZIONALE CHE        USCISSE CON k=1 CHE QUINDI PROBABILMENTE ERA UNA E UNA SOLA VARIABILE DELL'UNICO PUNTO VICINO A FARE LA DIFFERENZA
mat.2[i,j+2]<-mean(knn.pred == labels.train[split ==j])
}
train.cv.tree <- transformed.glass.train[split != j,] #ho provato con quelli normalizzati ma non cambia nulla
test.cv.tree  <-transformed.glass.train[split == j,]
tree.glass.cv <- tree(Type ~. , train.cv.tree)
#metodo alberi
for (l in 1:tree.size.max){
pruned.tree <-prune.tree(tree.glass.cv ,method = "misclass", k = pruned.tree.k[l+1] )
tree.pred <-predict(pruned.tree , test.cv.tree[,-k.train] , type = "class")
mat.2[knn.kmax+l,j+2]<- mean(tree.pred == labels.train[split ==j])
}
train.maha <- transformed.glass.train[split != j,]
test.maha <-transformed.glass.train[split ==j,]
#metodo knn maha
for (h in 1:knn.mahalanobis.max){
mat.2[knn.kmax+tree.size.max+h,j+2]<- knn_mahalanobis(train.maha[,-k.train], test.maha[,-k.train], train.maha$Type, test.maha$Type, k=h)
}
}
res.2<-cbind(mat.2[,c(1,2)],apply(mat.2[,3:j+2],MARGIN=1,mean))
colnames(res.2)<-c("metodo","hyperparameter","accuracy")
res.2<-as.tibble(res.2)
best.res.2<-rbind(res.2%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
res.2%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)),
res.2%>%filter(metodo=="knn.maha")%>%filter(accuracy==max(accuracy)))
best.res.2
res.2 %>%
ggplot(mapping  = aes(x = hyperparameter, y= accuracy, color=metodo))  +
geom_line()+
geom_point()+
facet_grid(~metodo) +
labs(title= "Variazione di Accuracy Rispetto a Iperparametri") #DA CAMBIARE IL TITOLO....
knitr::opts_chunk$set(echo = T)
knitr::opts_chunk$set(comment = NA)
library(tidyverse)
library(mclust)
library(Rmixmod)
library(GGally)
library(caret)
library("tree")
library(ggcorrplot)
library(class)
library(nnet)
library(car)
#install.packages("hunspell")
path <- "glass.csv"
glass <- tibble(read.table(path , sep = "," , header = T))
glass$Type <- factor(glass$Type)
n<-nrow(glass)
k<-ncol(glass)
#implementiamo una funzione per train/test
dataset_division <- function(D1, perc = 0.8) {
set.seed(42)
id <- sample(size = round(perc* nrow(D1)) , x = nrow(D1))
train <- D1[id,]
test <- D1[-id,]
out <- list(train = train, test = test)
}
division<-dataset_division(glass)
glass.train <- division$train
glass.test <- division$test
# glass.train%>%
#   select(-Type) %>%
#     ggpairs(mapping = aes(color = glass.train$Type))
ggcorrplot(cor(glass.train%>%
select(-Type)), lab = T , type = "lower" ) +
labs(title = "Plot di Correlazioni Lineari") #FAREI I GRAFICI SU "Glass" intero....(e metterei la divisione train/test dopo tutti i grafici)
#questo bar plot è riguardo freq relative
glass.train %>%
ggplot(aes(x= Type, y = after_stat(count)/sum(after_stat(count)), fill = Type)) +
geom_bar() +
labs(title = "Frequenza Relativa delle Classi",y= "Freq Rel")
#questo bar plot è riguardo freq assolute
glass.train %>%
ggplot(aes(x= Type, y = after_stat(count), fill = Type)) +
geom_bar() +
labs(title = "Frequenza Assoluta delle Classi",y= "Freq Ass")
#MA LI METTIAMO ENTRAMBI?
glass.train %>%
select(-Type)%>%
gather(key = tipo, value=valore)%>%
ggplot(mapping= aes(y = valore)) +
geom_boxplot()+
facet_wrap(~ tipo,scales="free")+
labs(title = "Box Plot con Tutte le Variabili")
#qua devo ancora capire come farlo meglio sarebbero i box plot condizionati e ad ogni misurazione ad ogni tipo il problema è la scala di gradezza è diversa per ogni casella
glass.train %>%
gather(key = Measure, value = valore, -Type )  %>%
ggplot(aes(x = Type, y= valore, color = Measure)) +
geom_violin(trim = F)+
geom_boxplot(width=0.1) +
facet_wrap( ~ Measure ,scales = "free", ncol =3)+
labs(title = "Violin plot condizionato alle classi")
glass.train %>%
filter(Type == "6")
glass.train %>%
filter(if_any("RI" : "Fe", ~ . > 0 ))
glass.train %>%
gather(key = Measure, value = valore, -Type )  %>%
ggplot(aes(x= valore , color = Type)) +
geom_density() +
facet_wrap(~Measure, scales= "free")
p.train <-glass.train %>%
select( -c("Fe", "K" , "Ba", "Type"))
p.test <-glass.test %>%
select( -c("Fe", "K" , "Ba", "Type"))
p.exp <- p.train %>%
apply(MARGIN =2 , FUN = powerTransform, family = "bcnPower")
a3 <- as.vector(c( p.exp$RI[1]$lambda,p.exp$Na[1]$lambda , p.exp$Mg[1]$lambda, p.exp$Al[1]$lambda, p.exp$Si[1]$lambda,p.exp$Ca[1]$lambda))
transformed.train <- bcPower(with(p.train, cbind(RI, Na, Mg, Al, Si, Ca)),
lambda = a3 ,gamma= 0.01)
transformed.test <- bcPower(with(p.test, cbind(RI, Na, Mg, Al, Si, Ca)),
lambda = a3 ,gamma= 0.01)
transformed.glass.train <- tibble(cbind(as.data.frame(transformed.train)), Type = glass.train$Type)
colnames(transformed.glass.train)<-c("RI_new","Na_new","Mg_new","Al_new","Si_new","Ca_new","Type")
transformed.glass.test <- tibble(cbind(as.data.frame(transformed.test)), Type = glass.test$Type)
colnames(transformed.glass.test)<-c("RI_new","Na_new","Mg_new","Al_new","Si_new","Ca_new","Type")
transformed.glass.train%>%
gather(key = Measure, value = valore, -Type )  %>%
ggplot(aes(x= valore , colour= Type)) +
geom_density() +
facet_wrap(~Measure, scales= "free")
#impostazione tabella pvalues
pvalue.shapiro <- matrix(0, nrow = (dim(glass.train)[2]-1), ncol = 6)
rownames(pvalue.shapiro) = colnames(glass.train)[-10]
colnames(pvalue.shapiro) = levels(glass.train$Type)
#-----
# Test Shapiro e costruzione di una matrice riassuntiva con i p-value condizionati alla classe
for (i in colnames(glass.train)[-10]){
for (j in levels(glass.train$Type)){
pvalue.shapiro[i,j]<-
shapiro.test( jitter(glass.train %>% filter(Type == j)%>%pull(i)) )$p.value #qua si assegnano tutti i valori, bisonga stare attenti perchè alcuni di questi non sono validi perchè ho messo jitter non lo fa andare in errore quando tutti i valori sono uguali ma introduce noise gaussiano che rende normali quando non sono ugugali
# pull invece serve per tirare fuori dal tibble un vettore (e non un tibble)
}
}
pvalue.shapiro["Ca", "6" ] <- pvalue.shapiro["K", "6" ] <- pvalue.shapiro["Fe", "6" ] <- NA
round(pvalue.shapiro, 5)
#qua si crea tabella pvalues
pvalue.shapiro.2 <- matrix(0, nrow = (dim(transformed.glass.train)[2]-1), ncol = 6)
rownames(pvalue.shapiro.2) = colnames(transformed.glass.train)[-7]
colnames(pvalue.shapiro.2) = levels(transformed.glass.train$Type)
#-----
# Test Shapiro e costruzione di una matrice riassuntiva con i p-value condizionati alla classe
for (i in colnames(transformed.glass.train)[-7]){
for (j in levels(glass.train$Type)){
pvalue.shapiro.2[i,j]<-
shapiro.test( jitter(transformed.glass.train %>% filter(Type == j)%>%pull(i)) )$p.value #qua si assegnano tutti i valori, bisonga stare attenti perchè alcuni di questi non sono validi perchè ho messo jitter non lo fa andare in errore quando tutti i valori sono uguali ma introduce noise gaussiano che rende normali quando non sono ugugali
# pull invece serve per tirare fuori dal tibble un vettore (e non un tibble)
}
}
round(pvalue.shapiro.2, 5)
#v è un vettore colonna il primo valore è il minimo il secondo il massimo e il resto solo i valori effettivi delle colonne (Fe, Ba, etc)
#funzione che prende vettore, toglie il primo numero, e lo divide da secondo - primo
func<-function(v){
return((v-v[1])/(v[2]-v[1])) # non aggiorno v[1] v[2] fino alla fine e quelli saranno innutili
}
# matrix sarà la nostra tabella min e max
norm_aux<-function(data,matrix){
data_2<-rbind(matrix,data)
data_norm<-apply(data_2,MARGIN = 2,func) #applichiamo func ad ogni colonna
return(data_norm[3:nrow(data_norm),]) # da 3 in poi perchè i primi due valori sono il minimo e il massimo utilizzati per normalizzare
}
# questa funzione prende sia il train che il test e normalizza il test con min e max del train
normalize<-function(train,test){
matrix_min_max<-rbind(apply(train,MARGIN = 2,min),apply(train,MARGIN = 2,max)) #qua salviamo min e max -> matrice che anda in norm aux
train_norm<-norm_aux(train,matrix_min_max)
test_norm<-norm_aux(test,matrix_min_max)
return(list(train_norm=train_norm,test_norm=test_norm))
}
#qua ho fatto un albero e sembra buono
set.seed(47)
tree.glass <- tree(Type ~. , glass.train)
tree.pred <- predict(tree.glass , glass.test[-10] , type = "class")
plot(tree.glass); text(tree.glass, , pretty = 0) # albero completo # queste due linee vanno eseguite insieme
mean(tree.pred == glass.test$Type)
pruned.tree.k <-prune.tree(tree.glass  ,method="misclass")$k
mean(tree.pred == glass.test$Type) #VEDERE DOVE INSERIRE QUESTO VALORE PERCHE SOPRA VIENE VISUALIZZATO SOLO IL PLOT...???
Kmax<-20
out<-normalize(glass.train[,-10],glass.test[,-10])
set.seed(101)
knn.pred.k<-rep(0,Kmax)
X_train<- out$train_norm
labels<-glass.train$Type
X_test<- out$test_norm
for (i in 1:Kmax){
knn.pred <- knn(train=X_train, test=X_test, cl=labels, k = i)
knn.pred.k[i]<-mean(knn.pred == glass.test$Type)
}
r<-rep("black",Kmax)
r[which.max(knn.pred.k)]<-"red"
plot(1:Kmax,knn.pred.k,type="h",ylim=c(0,1),col=r)
abline(h=max(knn.pred.k),col="purple")
#si prova con mahalanobis distance
mahalanobis_dist <- function(x, mean_vec, sigma_inv) {
diff_vec <- t(t(x) - mean_vec)
out <- sqrt(as.numeric(t(diff_vec) %*% sigma_inv %*% diff_vec ))
return(out)
}
find_mode <- function(x) {
u <- unique(x)
tab <- tabulate(match(x, u))
u[tab == max(tab)]
}
predict_label <- function(v, train, mat, k, train_lab) {
dists <- apply(train, MARGIN = 1, function(x) mahalanobis_dist(x, v, mat))
check <- data.frame("dist"=dists,"lab"=train_lab)
check <- as.tibble(check)
check_ord <-  check  %>% slice_min(dist, n = k)
return(find_mode(check_ord$lab))
}
# si implementa una funzione per l'applicazione del metodo knn tramite distanza di mahalanobis
knn_mahalanobis <- function(train, valid, train_lab, valid_lab, k) {
x_matrix_inv <- solve(cor(train))
pred <- apply(valid, MARGIN = 1, function(x) predict_label(x, train, x_matrix_inv, k, train_lab))
out <- mean(pred == valid_lab)
return(out)
}
Kmax<-40
#non si normalizza perchè fa già mahalanobis????
set.seed(47)
knn.pred.k<-rep(0,Kmax)
X_train<-glass.train[,-10]
labels.train<-glass.train$Type
labels.test<-glass.test$Type
X_test<-glass.test[,-10]
for (i in 1:Kmax){
knn.pred.k[i]<-knn_mahalanobis(X_train, X_test, labels.train, labels.test, k=i)
}
r<-rep("black",Kmax)
r[which.max(knn.pred.k)]<-"red"
plot(1:Kmax,knn.pred.k,type="h",ylim=c(0,1),col=r)
abline(h=max(knn.pred.k),col="purple")
#teniamo senza normalizzare
cv<-5
p<-1-1/cv
n.train<-nrow(glass.train)
k.train <-ncol(glass.train)
labels.train <-glass.train$Type
knn.kmax<-10
tree.size.max<- length(pruned.tree.k)-2
knn.mahalanobis.max<-40
mat<-as.data.frame(matrix(NA,nrow=(knn.kmax+tree.size.max+knn.mahalanobis.max),ncol=cv+2))
mat[1:knn.kmax,1]<-"knn"
mat[(knn.kmax+1):(knn.kmax+tree.size.max),1]<-"tree"
mat[(knn.kmax+tree.size.max+1):(knn.kmax+tree.size.max+knn.mahalanobis.max),1]<-"knn.maha"
mat[1:knn.kmax,2]<-1:knn.kmax
mat[(knn.kmax+1):(knn.kmax+tree.size.max),2]<- pruned.tree.k[2:(length(pruned.tree.k)-1)]
mat[(knn.kmax+tree.size.max+1):(knn.kmax+tree.size.max+knn.mahalanobis.max),2]<- 1:knn.mahalanobis.max
set.seed(47)
group <- rep(c(1:cv), round(n.train/cv))
random.assign <- sample( c(1:cv),size = n.train - length(group)) # ne mancheranno sempre cv o meno quindi non ci serve replace
split<-sample( append(group, random.assign) , size=n.train)
#loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv
for (j in 1:cv){
out.norm.cv<-normalize(glass.train[split != j  ,-k.train] ,glass.train[split ==j ,-k.train])
train.cv<-out.norm.cv$train_norm
test.cv<-out.norm.cv$test_norm
#metodo knn
for (i in 1:knn.kmax){
knn.pred <- knn(train=train.cv, test=test.cv, cl=labels.train[split != j], k = i)
#knn.pred <- knnMCN(train=train_cv, test=test_cv, TstX = NULL, K = i, ShowObs = F)
#TENIAMO PRESENTE CHE FACENDO NORMALIZE IN QUESTO MODO IL     RUSLTATO CAMBIA TOTALMENTE E PASSA DA k=1 a k=4 CHE HA SENSO??? O FORSE CAMBIO DRASTICO...NORMALIZZARE COME AVEVO FATTO PRIMA       ERA SBAGLIATO E QUESTO MIGLIORAMENTO LO ATTESTA (CON E SENZA NORMALIZZAZIONE C'E UN CAMBIAMENTO BELLO STRONG). RAZIONALE CHE        USCISSE CON k=1 CHE QUINDI PROBABILMENTE ERA UNA E UNA SOLA VARIABILE DELL'UNICO PUNTO VICINO A FARE LA DIFFERENZA
mat[i,j+2]<-mean(knn.pred == labels.train[split ==j])
}
train.cv.tree <- glass.train[split != j,]
test.cv.tree  <-glass.train[split ==j,]
tree.glass.cv <- tree(Type ~. , train.cv.tree)
#metodo alberi
for (l in 1:tree.size.max){
pruned.tree <-prune.tree(tree.glass.cv ,method = "misclass", k = pruned.tree.k[l+1] )
tree.pred <-predict(pruned.tree , test.cv.tree[,-k.train] , type = "class")
mat[knn.kmax+l,j+2]<- mean(tree.pred == labels.train[split ==j])
}
train.maha <- glass.train[split != j,]
test.maha <-glass.train[split ==j,]
#metodo knn maha
for (h in 1:knn.mahalanobis.max){
mat[knn.kmax+tree.size.max+h,j+2]<- knn_mahalanobis(train.maha[,-k.train], test.maha[,-k.train], train.maha$Type, test.maha$Type, k=h)
}
}
res<-cbind(mat[,c(1,2)],apply(mat[,3:j+2],MARGIN=1,mean))
colnames(res)<-c("metodo","hyperparameter","accuracy")
res<-as.tibble(res)
best.res<-rbind(res%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
res%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)),
res%>%filter(metodo=="knn.maha")%>%filter(accuracy==max(accuracy)))
best.res
res %>%
ggplot(mapping  = aes(x = hyperparameter, y= accuracy, color=metodo))  +
geom_line()+
geom_point()+
facet_grid(~metodo) +
labs(title= "Variazione di Accuracy Rispetto a Iperparametri")
#dal grafico temo k=1 di mahalanobis sia da escludere....
cv<-5
p<-1-1/cv
n.train<-nrow(transformed.glass.train)
k.train <-ncol(transformed.glass.train)
labels.train <-transformed.glass.train$Type
knn.kmax<-10
tree.size.max<- length(pruned.tree.k)-2
knn.mahalanobis.max<-40
mat.2<-as.data.frame(matrix(NA,nrow=(knn.kmax+tree.size.max+knn.mahalanobis.max),ncol=cv+2))
mat.2[1:knn.kmax,1]<-"knn"
mat.2[(knn.kmax+1):(knn.kmax+tree.size.max),1]<-"tree"
mat.2[(knn.kmax+tree.size.max+1):(knn.kmax+tree.size.max+knn.mahalanobis.max),1]<-"knn.maha"
mat.2[1:knn.kmax,2]<-1:knn.kmax
mat.2[(knn.kmax+1):(knn.kmax+tree.size.max),2]<- pruned.tree.k[2:(length(pruned.tree.k)-1)]
mat.2[(knn.kmax+tree.size.max+1):(knn.kmax+tree.size.max+knn.mahalanobis.max),2]<- 1:knn.mahalanobis.max
set.seed(47)
group <- rep(c(1:cv), round(n.train/cv))
random.assign <- sample( c(1:cv),size = n.train - length(group)) # ne mancheranno sempre cv o meno quindi non ci serve replace
split<-sample( append(group, random.assign) , size=n.train) #non ha senso questo metodo, capisco avere stessa proporzione ma usiamo una tecnica più flessibile se modifichiamo cv o proporzione train/test all'inizio (30% nel test set mi sembra eccessivo)
#loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv
for (j in 1:cv){
out.norm.cv.2<-normalize(transformed.glass.train[split != j  ,-k.train] ,transformed.glass.train[split ==j ,-k.train])
train.cv<-out.norm.cv.2$train_norm
test.cv<-out.norm.cv.2$test_norm
#metodo knn
for (i in 1:knn.kmax){
knn.pred <- knn(train=train.cv, test=test.cv, cl=labels.train[split != j], k = i)
#knn.pred <- knnMCN(train=train_cv, test=test_cv, TstX = NULL, K = i, ShowObs = F)
#TENIAMO PRESENTE CHE FACENDO NORMALIZE IN QUESTO MODO IL     RUSLTATO CAMBIA TOTALMENTE E PASSA DA k=1 a k=4 CHE HA SENSO??? O FORSE CAMBIO DRASTICO...NORMALIZZARE COME AVEVO FATTO PRIMA       ERA SBAGLIATO E QUESTO MIGLIORAMENTO LO ATTESTA (CON E SENZA NORMALIZZAZIONE C'E UN CAMBIAMENTO BELLO STRONG). RAZIONALE CHE        USCISSE CON k=1 CHE QUINDI PROBABILMENTE ERA UNA E UNA SOLA VARIABILE DELL'UNICO PUNTO VICINO A FARE LA DIFFERENZA
mat.2[i,j+2]<-mean(knn.pred == labels.train[split ==j])
}
train.cv.tree <- transformed.glass.train[split != j,] #ho provato con quelli normalizzati ma non cambia nulla
test.cv.tree  <-transformed.glass.train[split == j,]
tree.glass.cv <- tree(Type ~. , train.cv.tree)
#metodo alberi
for (l in 1:tree.size.max){
pruned.tree <-prune.tree(tree.glass.cv ,method = "misclass", k = pruned.tree.k[l+1] )
tree.pred <-predict(pruned.tree , test.cv.tree[,-k.train] , type = "class")
mat.2[knn.kmax+l,j+2]<- mean(tree.pred == labels.train[split ==j])
}
train.maha <- transformed.glass.train[split != j,]
test.maha <-transformed.glass.train[split ==j,]
#metodo knn maha
for (h in 1:knn.mahalanobis.max){
mat.2[knn.kmax+tree.size.max+h,j+2]<- knn_mahalanobis(train.maha[,-k.train], test.maha[,-k.train], train.maha$Type, test.maha$Type, k=h)
}
}
res.2<-cbind(mat.2[,c(1,2)],apply(mat.2[,3:j+2],MARGIN=1,mean))
colnames(res.2)<-c("metodo","hyperparameter","accuracy")
res.2<-as.tibble(res.2)
best.res.2<-rbind(res.2%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
res.2%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)),
res.2%>%filter(metodo=="knn.maha")%>%filter(accuracy==max(accuracy)))
best.res.2
res.2 %>%
ggplot(mapping  = aes(x = hyperparameter, y= accuracy, color=metodo))  +
geom_line()+
geom_point()+
facet_grid(~metodo) +
labs(title= "Variazione di Accuracy Rispetto a Iperparametri") #DA CAMBIARE IL TITOLO....
k<-7
out.norm<- normalize(transformed.glass.train[,-k],transformed.glass.test[,-k])
train.norm<-out.norm$train_norm
test.norm<-out.norm$test_norm
labels.train<-transformed.glass.train$Type
set.seed(47)
knn.pred.transf <- knn(train=train.norm, test=test.norm, cl=labels.train, k = 3) #non funziona se ci escono 2 o più soluzioni
#TOCCA SCEGLIERE IL SEED E USIAMO QUEL CHE CI ESCE
mean(transformed.glass.test$Type == knn.pred.transf) #molto più alto rispetto al validation...temo che usare solo 214 record è poco...
reference<-transformed.glass.test$Type #VEDETE SE INSERIRE, PER ME CI STA
pred.best<-knn.pred.transf
confusionMatrix(data = pred.best,reference = reference, dnn = c("predicted values","true values")) #scegliete se value o values...
