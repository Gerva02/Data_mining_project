---
title: "Progetto "
author: "Simone Gervasoni Angelo Cardinale Riccardo Bonamoni "
date: "2024-04-05"
output:
  html_document:
    self_contained : TRUE
    toc : TRUE
    toc_depth: 2 
    number_sections: yes
    toc_float: 
      collapsed: no
      smooth_scroll: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
knitr::opts_chunk$set(comment = NA)
```

```{r librerie, warning=FALSE, message= FALSE }
library(tidyverse)
library(mclust)
library(Rmixmod)
library(GGally)
library(caret)
library("tree")
library(ggcorrplot)
library(class)
library(nnet)
library(car)
#install.packages("hunspell")
```

# Abstract

In questo progetto viene analizzato un dataset contenente diverse tipologie di vetri.
L'obiettivo è la classificazione di diversi vetri in studio, tramite l'analisi del loro indice di rifrazione e di alcuni elementi chimici presenti.
Nel lavoro verranno utilizzate e confrontate (TOT) tecniche di classificazione ovvero (…).


# Introduzione

Il vetro è un materiale solido che viene detto amorfo, in quanto si comporta come un solido a temperatura ambiente, ma possedendo a livello microscopico una struttura disordinata e rigida, al contrario della struttura ordinata dei normali solidi. Il vetro deriva infatti dalla solidificazione di un liquido.
I normali vetri sono prevalentemente costituiti da ossido di silicio, ma spesso in fase di produzione vengono aggiunte altre sostanze che ne vanno a modificare le proprietà e le funzioni. 

La previsione della tipologia di certi vetri può essere rilevante in alcuni ambiti. 
I dati utilizzati in questo progetto erano infatti stati raccolti con la finalità di indagini di criminologia. Tramite lo studio della classificazione del tipo di vetro è infatti possibile ottenere diverse evidenze. Ad esempio, un bicchiere sulla scena di un crimine può essere usato come una prova.
Altre ricerche, della University of California, sono volte, tramite la classificazione, a determinare se un determinato vetro sia float o meno.
Il vetro float, o vetro galleggiante, è infatti ad oggi il vetro più prodotto e utilizzato sul mercato. Grazie alle sue qualità è infatti ideale per diverse applicazioni come vetri per automobili, specchi, finestre o porte, ma anche per la produzione di vetri più specializzati come quello temperato, smerigliato, laminato o insonorizzato.

Il fine del progetto è quindi quello di trovare modelli che classifichino, con maggiore precisione possibile, la tipologia di ciascun vetro, data la sua composizione e analisi chimica.

QUI POTREMO DIRE BREVEMENTE I METODI UTILIZZATI E SOPRATTUTO QUELLI NON USATIE PERCHè


# Materiali

Il dataset impiegato, *Glass Classification* è stato scaricato dalla piattaforma Kaggle. Proviene dalla USA Forensic Science Service ed è messo a disposizione anche dalla University of California, le quali lo hanno utilizzato per le indagini e analisi di cui sopra.

La matrice dei dati è composta da 214 osservazioni e 10 variabili, riportate di seguito.

* RI: l'indice di rifrazione del vetro, una grandezza adimensionale
* 8 elementi chimici, tutti espressi in percentuale in peso all'interno dell'ossido corrispondente.

  * Na: Sodio
  * Mg: Magnesio
  * Al: Alluminio
  * Si: Silicio
  * K: Potassio
  * Ca: Calcio
  * Ba: Bario
  * Fe: Ferro
  
* Type : La tipologia di vetro, è la __variabile target__. È suddivisa in 7 classi.

  1. building_windows_float_processed
  2. building_windows_non_float_processed
  3. vehicle_windows_float_processed
  4. vehicle_windows_non_float_processed 
  5. containers
  6. tableware
  7. headlamps

La variabile target è analizzata come un *Factor* in R. Per la classe 4 non sono presenti valori nel dataset.

Sono state effettuate analisi per il controllo di valori mancanti ed è stato riscontrato che non sono presenti rendendo quindi non necessarie tutte le tecniche volte all'imputazione o rimozione di essi.




# Preprocessing

## Analisi esplorativa 

```{r import e inizio }
path <- "glass.csv"
glass <- tibble(read.table(path , sep = "," , header = T))
glass$Type <- factor(glass$Type)
n<-nrow(glass)
k<-ncol(glass)
```

In questa fase di preprocessing, come primo step, è stato diviso il dataset in training e test. Il training rappresenta un 80% casuale dei nostri dati, mentre il test il restante 20%. Le analisi, sia descrittive che grafiche, svolte successivamente sono applicate solamente al dataset di training, escludendo momentaneamente il test set.

```{r division dataset}
#implementiamo una funzione per train/test
dataset_division <- function(D1, perc = 0.8) {
  set.seed(42)
  id <- sample(size = round(perc* nrow(D1)) , x = nrow(D1))
  train <- D1[id,]
  test <- D1[-id,]
  out <- list(train = train, test = test)
}

division<-dataset_division(glass)
glass.train <- division$train 
glass.test <- division$test 

```

Innanzitutto, è rappresentata la matrice di correlazione tra tutte le nostre variabili, tutte quantitative, esclusa ovviamente la variabile target. Si nota una forte correlazione positiva tra Ca e RI.
((((( per alberi non fa niente ma per Knn andrà vista ))))

```{r ggcorplots}
# glass.train%>%
#   select(-Type) %>%
#     ggpairs(mapping = aes(color = glass.train$Type))  
ggcorrplot(cor(glass.train%>%
 select(-Type)), lab = T , type = "lower" ) +
  labs(title = "Plot di Correlazioni Lineari") #FAREI I GRAFICI SU "Glass" intero....(e metterei la divisione train/test dopo tutti i grafici)
```

Successivamente sono visualizzati i due istogrammi relativi alle frequenze, rispettivamente, relative e assolute delle 7 classi della variabile target *Type*. Si ricorda che la quarta classe non è presente, riducendo a 6 classi la nostra analisi.

Si nota facilmente come le prime due classi siano molto più presenti delle altre. Infatti, solo queste due rispecchiano circa il 65% di tutte le osservazioni.
Questa situazione si identifica nel problema di classi sbilanciate, ovvero quando nel set di dati di addestramento (training set) i dati non ugualmente distribuiti tra le classi, portando a possibili maggiori errori predittivi.
In questi contesti è solito cercare di ribilanciare le classi, tramite tecniche di oversampling (SMOTE) o undersampling, in particolare se si ritiene di una certa importanza inidviduare determinate classi meno presenti.
Tuttavia, successivamente ad alcune ricerche, non è stato ritenuto opportuno applicare queste tecniche al dataset in analisi. Infatti, non è presente nessuna classe di tipologia di vetri più rilevante rispetto ad altre.

```{r barplot}
#questo bar plot è riguardo freq relative
glass.train %>%
  ggplot(aes(x= Type, y = after_stat(count)/sum(after_stat(count)), fill = Type)) + 
  geom_bar() +
  labs(title = "Frequenza Relativa delle Classi",y= "Freq Rel")
 
#questo bar plot è riguardo freq assolute
glass.train %>%
  ggplot(aes(x= Type, y = after_stat(count), fill = Type)) + 
  geom_bar() +
  labs(title = "Frequenza Assoluta delle Classi",y= "Freq Ass")

#MA LI METTIAMO ENTRAMBI?
```

Vengono di seguito riportati i boxplot di ciascuna delle variabili, senza aver effettuato nessun tipo di trasformazione. Si nota una distribuzione abbastanza sbilanciata, con la presenza di una forte asimmetria nelle variabili *Ba*, *Fe* e *K*. Le restanti variabili sembrano invece più simmetriche e senza la presenza di outlier. 
Questi valori potrebbero essere problematici al fine delle analisi e verranno ripresi e trattati successivamente.

```{r}
glass.train %>% 
  select(-Type)%>%
  gather(key = tipo, value=valore)%>%
  ggplot(mapping= aes(y = valore)) +
  geom_boxplot()+
  facet_wrap(~ tipo,scales="free")+
  labs(title = "Box Plot con Tutte le Variabili")
  
```

È utile osservare anche il Violin plot delle variabili condizionato alle classi. Ciò permette di vedere una stima della densità non parametrica delle distribuzioni di ogni variabile rispetto alle classi. 
Tramite questo grafico si riescono infatti a fare 3 principali osservazioni:

* La varianza e la scala delle variabili sono diverse tra di loro: questo porterà a effettuare una normalizzazione, utile per esempio quando verrà applicato l'algoritmo K-NN.
* La varianza per ogni classe per ognuna delle variabili è diversa: questo porta ad escludere l'utilizzo dell'analisi discriminante lineare (LDA), in quanto ipotesi necessaria di essa. 
* La distribuzione condizionata per ogni variabili non sembra essere quasi mai normale: viene quindi rifiutata anche l'analisi discriminante quadratica (QDA), oltre a LDA. Si conferma inoltre l'asimmetria. 


```{r}
#qua devo ancora capire come farlo meglio sarebbero i box plot condizionati e ad ogni misurazione ad ogni tipo il problema è la scala di gradezza è diversa per ogni casella
glass.train %>%
  gather(key = Measure, value = valore, -Type )  %>%
  ggplot(aes(x = Type, y= valore, color = Measure)) +
  geom_violin(trim = F)+ 
  geom_boxplot(width=0.1) + 
  facet_wrap( ~ Measure ,scales = "free", ncol =3)+
  labs(title = "Violin plot condizionato alle classi") 
```

Si nota inoltre tramite il seguente tibble che, per la classe Type = 6, le variabili *Ba*, *Fe* e *K* non variano e sono tutte pari a zero. Si tratta degli stessi 3 elementi chimici che presentano la distribuzione più asimmetrica e verranno in seguito trattati.

```{r}
glass.train %>%
  filter(Type == "6")
```

Di seguito è riportata una verifica dell'assenza di osservazioni nel training set con valori tutti pari a zero: ciò viene confermato dal tibble di 171 osservazioni, ovvero l'80% del dataset completo.

```{r}
glass.train %>%
  filter(if_any("RI" : "Fe", ~ . > 0 ))
```

Si riportano infine i grafici della stima delle distribuzioni di ogni variabile, condizionata ad ognuna delle 6 classi presenti. Questi grafici confermano i medesimi risultati e osservazioni dei violin plot visualizzati precedentemente in maniera differente.


```{r}
glass.train %>%
  gather(key = Measure, value = valore, -Type )  %>%
  ggplot(aes(x= valore , color = Type)) +
  geom_density() +
  facet_wrap(~Measure, scales= "free")
```


## Trasformazione variabili

In una seconda fase del preprocessing si procede con delle trasformazioni opportune delle variabili.
La trasformazione applicata è stata quella di Box-Cox dato che si tratta di una trasformazione molto completa e utile che include sia il logaritmo che le trasformazioni potenza, in base ad un parametro $\lambda$.
Si è deciso di non trasformare le 3 variabili *Ba*, *Fe* e *K* analizzate precedentemente. Queste presentavano valori non solo asimmetrici, ma molto spostati verso lo zero, rendendo quindi ininfluenti e inutili le trasformazioni. 

Il dataset è stato quindi diviso in due gruppi:

* Il primo contiene tutte le 9 variabili non trasformate
* Il secondo ha invece le 6 variabili trasformate, con l'esclusione delle 3 sopraccitate.

Di seguito vengono riportati i grafici della stima della densità non parametrica delle distribuzioni delle nuove 6 variabili trasformate, rispetto alle classi. Il risultato è buono in quanto tutte le variabili sembrano avere distribuzione più normale.

```{r}
p.train <-glass.train %>% 
  select( -c("Fe", "K" , "Ba", "Type"))
p.test <-glass.test %>%
  select( -c("Fe", "K" , "Ba", "Type"))
p.exp <- p.train %>% 
  apply(MARGIN =2 , FUN = powerTransform, family = "bcnPower")  
a3 <- as.vector(c( p.exp$RI[1]$lambda,p.exp$Na[1]$lambda , p.exp$Mg[1]$lambda, p.exp$Al[1]$lambda, p.exp$Si[1]$lambda,p.exp$Ca[1]$lambda))

transformed.train <- bcPower(with(p.train, cbind(RI, Na, Mg, Al, Si, Ca)),
                lambda = a3 ,gamma= 0.01)
transformed.test <- bcPower(with(p.test, cbind(RI, Na, Mg, Al, Si, Ca)),
                lambda = a3 ,gamma= 0.01)

transformed.glass.train <- tibble(cbind(as.data.frame(transformed.train)), Type = glass.train$Type)
colnames(transformed.glass.train)<-c("RI_new","Na_new","Mg_new","Al_new","Si_new","Ca_new","Type")
transformed.glass.test <- tibble(cbind(as.data.frame(transformed.test)), Type = glass.test$Type)
colnames(transformed.glass.test)<-c("RI_new","Na_new","Mg_new","Al_new","Si_new","Ca_new","Type")

transformed.glass.train%>%
  gather(key = Measure, value = valore, -Type )  %>%
  ggplot(aes(x= valore , colour= Type)) +
  geom_density() +
  facet_wrap(~Measure, scales= "free")

```

Si procede con lo studio della normalità tramite appositi test.
Viene utilizzato il Test di Shapiro-Wilk che verifica tramite l'ipotesi nulla la normalità del campione.
Sono di seguito riportate le due tabelle per ognuno dei due gruppi:

* Nella prima ci sono i p-value del test per ognuna delle 9 variabili iniziali non trasformate, rispetto ad ogni classe: è imputato un valore NA per i 3 elementi chimici con tutti valori pari a zero nella classe 6.
* Nella seconda sono presenti i p-value del test solamente delle 6 variabili trasformate.

In entrambi i casi si osserva che i p-value sono quasi tutti molto "bassi" facendo rifiutare l'ipotesi nulla di normalità. Vengono quindi confermati analiticamente i risultati ottenuti prima tramite le analisi grafiche, portando all'esclusione di LDA e QDA.

```{r shapiro test}
#impostazione tabella pvalues
pvalue.shapiro <- matrix(0, nrow = (dim(glass.train)[2]-1), ncol = 6)
rownames(pvalue.shapiro) = colnames(glass.train)[-10]
colnames(pvalue.shapiro) = levels(glass.train$Type)
#-----

# Test Shapiro e costruzione di una matrice riassuntiva con i p-value condizionati alla classe
for (i in colnames(glass.train)[-10]){
  for (j in levels(glass.train$Type)){
    pvalue.shapiro[i,j]<-
      shapiro.test( jitter(glass.train %>% filter(Type == j)%>%pull(i)) )$p.value #qua si assegnano tutti i valori, bisonga stare attenti perchè alcuni di questi non sono validi perchè ho messo jitter non lo fa andare in errore quando tutti i valori sono uguali ma introduce noise gaussiano che rende normali quando non sono ugugali
    # pull invece serve per tirare fuori dal tibble un vettore (e non un tibble)
  }
}

pvalue.shapiro["Ca", "6" ] <- pvalue.shapiro["K", "6" ] <- pvalue.shapiro["Fe", "6" ] <- NA

round(pvalue.shapiro, 5)
```

```{r shapiro test dati trasformati}
#qua si crea tabella pvalues
pvalue.shapiro.2 <- matrix(0, nrow = (dim(transformed.glass.train)[2]-1), ncol = 6)
rownames(pvalue.shapiro.2) = colnames(transformed.glass.train)[-7]
colnames(pvalue.shapiro.2) = levels(transformed.glass.train$Type)
#-----

# Test Shapiro e costruzione di una matrice riassuntiva con i p-value condizionati alla classe
for (i in colnames(transformed.glass.train)[-7]){
  for (j in levels(glass.train$Type)){
    pvalue.shapiro.2[i,j]<-
      shapiro.test( jitter(transformed.glass.train %>% filter(Type == j)%>%pull(i)) )$p.value #qua si assegnano tutti i valori, bisonga stare attenti perchè alcuni di questi non sono validi perchè ho messo jitter non lo fa andare in errore quando tutti i valori sono uguali ma introduce noise gaussiano che rende normali quando non sono ugugali
    # pull invece serve per tirare fuori dal tibble un vettore (e non un tibble)
  }
}

round(pvalue.shapiro.2, 5)
```

L'ultimo step della fase di Preprocessing è la normalizzazione. Sono di seguito implementate delle funzioni che vanno a normalizzare tutti i nostri dati. Ciò è essenziale per poter successivamente applicare l'algoritmo del  K-NN che utilizza le distanze euclidee.


```{r normalizzazione}
#v è un vettore colonna il primo valore è il minimo il secondo il massimo e il resto solo i valori effettivi delle colonne (Fe, Ba, etc)

#funzione che prende vettore, toglie il primo numero, e lo divide da secondo - primo 


func<-function(v){
  return((v-v[1])/(v[2]-v[1])) # non aggiorno v[1] v[2] fino alla fine e quelli saranno innutili
}

# matrix sarà la nostra tabella min e max
norm_aux<-function(data,matrix){
  data_2<-rbind(matrix,data)  
  data_norm<-apply(data_2,MARGIN = 2,func) #applichiamo func ad ogni colonna
  return(data_norm[3:nrow(data_norm),]) # da 3 in poi perchè i primi due valori sono il minimo e il massimo utilizzati per normalizzare
}

# questa funzione prende sia il train che il test e normalizza il test con min e max del train
normalize<-function(train,test){
  matrix_min_max<-rbind(apply(train,MARGIN = 2,min),apply(train,MARGIN = 2,max)) #qua salviamo min e max -> matrice che anda in norm aux
  train_norm<-norm_aux(train,matrix_min_max)
  test_norm<-norm_aux(test,matrix_min_max)
  return(list(train_norm=train_norm,test_norm=test_norm))
} 

```


# Metodi di classificazione

## Alberi

Come primo metodo è stato provato quello degli alberi decisionali. Questi infatti non hanno particolari assunzioni preliminari e possono quindi essere applicati al dataset in analisi senza nessun problema.
Viene riportato in figura l'albero completo T0 con tutti gli split presenti.
Questo albero fornisce un'accuratezza iniziale del 60.47 % sul test set e conferma quindi essere un buon modello da poter riutilizzare successivamente.

```{r alberi}
#qua ho fatto un albero e sembra buono
set.seed(47)

tree.glass <- tree(Type ~. , glass.train)
tree.pred <- predict(tree.glass , glass.test[-10] , type = "class")
plot(tree.glass); text(tree.glass, , pretty = 0) # albero completo # queste due linee vanno eseguite insieme
mean(tree.pred == glass.test$Type) 
pruned.tree.k <-prune.tree(tree.glass  ,method="misclass")$k

```

(((vediamo split inutili 

vediamo che con 11 leaf node è quello che predice meglio

5% improvement , but we can see that we have very little data for example class 6 has only 1 data point in test)))


```{r alberi da sistemare}
mean(tree.pred == glass.test$Type) #VEDERE DOVE INSERIRE QUESTO VALORE PERCHE SOPRA VIENE VISUALIZZATO SOLO IL PLOT...???
```


## KNN 

Un secondo possibile metodo è quello dei K-nearest neighbors (K-NN).
Per essere applicato è necessaria innanzitutto la normalizzazione di tutti i nostri dati, in modo tale da non avere risultati falsati a causa di possibili scale diverse nei valori. In questa prima fase vengono infatti utilizzate le distanze euclidee come criterio di misura.
Una seconda problematica è la forte multicollinerità che era stata trovata nella fase di preprocessing tra le variabili *Ca* e *RI*. A livello teorico una di queste due variabili andrebbe quindi omessa, però, a seguito di diverse analisi, è stato scelto di tenerle entrambe. Infatti provando a rimuovere una delle due o addirittura entrambi sono stati ottenuti risultati molto più scarsi rispetto al tenerle entrambe.

Dopo aver verificato e risolto tutte le problematiche relative alla verifica delle assunzioni si può quindi procedere alla ricerca del k ottimale, ricercato tra 1 e 20.
Tramite il test set si trova il k ottimale che va massimizzare l'accuracy. Viene riortato un grafico che va a mostrare per ogni k possibile la relativa media di osservazioni predette correttamente, individuando in k=8 il più efficiente. 





```{r knn 1}
Kmax<-20
out<-normalize(glass.train[,-10],glass.test[,-10])
set.seed(101)
knn.pred.k<-rep(0,Kmax)
X_train<- out$train_norm
labels<-glass.train$Type 
X_test<- out$test_norm
for (i in 1:Kmax){
  knn.pred <- knn(train=X_train, test=X_test, cl=labels, k = i)
  knn.pred.k[i]<-mean(knn.pred == glass.test$Type)
}
r<-rep("black",Kmax)
r[which.max(knn.pred.k)]<-"red"
plot(1:Kmax,knn.pred.k,type="h",ylim=c(0,1),col=r)
abline(h=max(knn.pred.k),col="purple")
```





```{r malanobis}

#si prova con mahalanobis distance

mahalanobis_dist <- function(x, mean_vec, sigma_inv) {
  diff_vec <- t(t(x) - mean_vec)
  out <- sqrt(as.numeric(t(diff_vec) %*% sigma_inv %*% diff_vec ))
  return(out)
}

find_mode <- function(x) {
  u <- unique(x)
  tab <- tabulate(match(x, u))
  u[tab == max(tab)]
}

predict_label <- function(v, train, mat, k, train_lab) {
  dists <- apply(train, MARGIN = 1, function(x) mahalanobis_dist(x, v, mat))
  check <- data.frame("dist"=dists,"lab"=train_lab)
  check <- as_tibble(check)
  check_ord <-  check  %>% slice_min(dist, n = k)
  return(find_mode(check_ord$lab))
}

# si implementa una funzione per l'applicazione del metodo knn tramite distanza di mahalanobis
knn_mahalanobis <- function(train, valid, train_lab, valid_lab, k) {
  x_matrix_inv <- solve(cor(train))
  pred <- apply(valid, MARGIN = 1, function(x) predict_label(x, train, x_matrix_inv, k, train_lab))
  out <- mean(pred == valid_lab)
  return(out)
}

```

```{r malanobis scelta k.best}
Kmax<-40
#non si normalizza perchè fa già mahalanobis????
set.seed(47)
knn.pred.k<-rep(0,Kmax)
X_train<-glass.train[,-10]
labels.train<-glass.train$Type 
labels.test<-glass.test$Type
X_test<-glass.test[,-10]
for (i in 1:Kmax){
  knn.pred.k[i]<-knn_mahalanobis(X_train, X_test, labels.train, labels.test, k=i) 
}
r<-rep("black",Kmax)
r[which.max(knn.pred.k)]<-"red"
plot(1:Kmax,knn.pred.k,type="h",ylim=c(0,1),col=r)
abline(h=max(knn.pred.k),col="purple")
#teniamo senza normalizzare
```



##Confronto metodi


```{r cv}
cv<-5
p<-1-1/cv
n.train<-nrow(glass.train)
k.train <-ncol(glass.train)
labels.train <-glass.train$Type
knn.kmax<-10
tree.size.max<- length(pruned.tree.k)-2
knn.mahalanobis.max<-40
mat<-as.data.frame(matrix(NA,nrow=(knn.kmax+tree.size.max+knn.mahalanobis.max),ncol=cv+2))
mat[1:knn.kmax,1]<-"knn"
mat[(knn.kmax+1):(knn.kmax+tree.size.max),1]<-"tree"
mat[(knn.kmax+tree.size.max+1):(knn.kmax+tree.size.max+knn.mahalanobis.max),1]<-"knn.maha"
mat[1:knn.kmax,2]<-1:knn.kmax
mat[(knn.kmax+1):(knn.kmax+tree.size.max),2]<- pruned.tree.k[2:(length(pruned.tree.k)-1)]
mat[(knn.kmax+tree.size.max+1):(knn.kmax+tree.size.max+knn.mahalanobis.max),2]<- 1:knn.mahalanobis.max


set.seed(47)
group <- rep(c(1:cv), round(n.train/cv))
random.assign <- sample( c(1:cv),size = n.train - length(group)) # ne mancheranno sempre cv o meno quindi non ci serve replace
split<-sample( append(group, random.assign) , size=n.train) 

#loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv
for (j in 1:cv){
  
  
  out.norm.cv<-normalize(glass.train[split != j  ,-k.train] ,glass.train[split ==j ,-k.train])
  train.cv<-out.norm.cv$train_norm
  test.cv<-out.norm.cv$test_norm
  #metodo knn
  for (i in 1:knn.kmax){
    knn.pred <- knn(train=train.cv, test=test.cv, cl=labels.train[split != j], k = i)
    #knn.pred <- knnMCN(train=train_cv, test=test_cv, TstX = NULL, K = i, ShowObs = F) 
    #TENIAMO PRESENTE CHE FACENDO NORMALIZE IN QUESTO MODO IL     RUSLTATO CAMBIA TOTALMENTE E PASSA DA k=1 a k=4 CHE HA SENSO??? O FORSE CAMBIO DRASTICO...NORMALIZZARE COME AVEVO FATTO PRIMA       ERA SBAGLIATO E QUESTO MIGLIORAMENTO LO ATTESTA (CON E SENZA NORMALIZZAZIONE C'E UN CAMBIAMENTO BELLO STRONG). RAZIONALE CHE        USCISSE CON k=1 CHE QUINDI PROBABILMENTE ERA UNA E UNA SOLA VARIABILE DELL'UNICO PUNTO VICINO A FARE LA DIFFERENZA
    mat[i,j+2]<-mean(knn.pred == labels.train[split ==j])
  }
    
  
  train.cv.tree <- glass.train[split != j,] 
  test.cv.tree  <-glass.train[split ==j,]
  tree.glass.cv <- tree(Type ~. , train.cv.tree)
  #metodo alberi
  for (l in 1:tree.size.max){
    pruned.tree <-prune.tree(tree.glass.cv ,method = "misclass", k = pruned.tree.k[l+1] )
    tree.pred <-predict(pruned.tree , test.cv.tree[,-k.train] , type = "class")
    mat[knn.kmax+l,j+2]<- mean(tree.pred == labels.train[split ==j])
  }
  
  train.maha <- glass.train[split != j,] 
  test.maha <-glass.train[split ==j,]
  #metodo knn maha
  for (h in 1:knn.mahalanobis.max){
    mat[knn.kmax+tree.size.max+h,j+2]<- knn_mahalanobis(train.maha[,-k.train], test.maha[,-k.train], train.maha$Type, test.maha$Type, k=h)
  }
  
  
}
res<-cbind(mat[,c(1,2)],apply(mat[,3:j+2],MARGIN=1,mean)) 
colnames(res)<-c("metodo","hyperparameter","accuracy")
res<-as_tibble(res)
best.res<-rbind(res%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
                res%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)),
                res%>%filter(metodo=="knn.maha")%>%filter(accuracy==max(accuracy)))
best.res
```

loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv


```{r grafico confronto}
res %>%
  ggplot(mapping  = aes(x = hyperparameter, y= accuracy, color=metodo))  +
  geom_line()+ 
  geom_point()+
  facet_grid(~metodo) +
  labs(title= "Variazione di Accuracy Rispetto a Iperparametri")
#dal grafico temo k=1 di mahalanobis sia da escludere....
```

bisogna far capire che gli iperparametri qua non sono sovraponibili per 2 ragioni 
più si va a destra per il knn meno il modello è variabile (è il contrario per gli alberi)
non stanno misurando la stessa cosa

## Confronto metodi con Transformed variables

```{r cv transformed variables}
cv<-5
p<-1-1/cv
n.train<-nrow(transformed.glass.train)
k.train <-ncol(transformed.glass.train)
labels.train <-transformed.glass.train$Type
knn.kmax<-10
tree.size.max<- length(pruned.tree.k)-2
knn.mahalanobis.max<-40
mat.2<-as.data.frame(matrix(NA,nrow=(knn.kmax+tree.size.max+knn.mahalanobis.max),ncol=cv+2))
mat.2[1:knn.kmax,1]<-"knn"
mat.2[(knn.kmax+1):(knn.kmax+tree.size.max),1]<-"tree"
mat.2[(knn.kmax+tree.size.max+1):(knn.kmax+tree.size.max+knn.mahalanobis.max),1]<-"knn.maha"
mat.2[1:knn.kmax,2]<-1:knn.kmax
mat.2[(knn.kmax+1):(knn.kmax+tree.size.max),2]<- pruned.tree.k[2:(length(pruned.tree.k)-1)]
mat.2[(knn.kmax+tree.size.max+1):(knn.kmax+tree.size.max+knn.mahalanobis.max),2]<- 1:knn.mahalanobis.max

set.seed(47)
group <- rep(c(1:cv), round(n.train/cv))
random.assign <- sample( c(1:cv),size = n.train - length(group)) # ne mancheranno sempre cv o meno quindi non ci serve replace
split<-sample( append(group, random.assign) , size=n.train) #non ha senso questo metodo, capisco avere stessa proporzione ma usiamo una tecnica più flessibile se modifichiamo cv o proporzione train/test all'inizio (30% nel test set mi sembra eccessivo)

#loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv
for (j in 1:cv){
  out.norm.cv.2<-normalize(transformed.glass.train[split != j  ,-k.train] ,transformed.glass.train[split ==j ,-k.train])
  train.cv<-out.norm.cv.2$train_norm
  test.cv<-out.norm.cv.2$test_norm
  #metodo knn
  for (i in 1:knn.kmax){
    knn.pred <- knn(train=train.cv, test=test.cv, cl=labels.train[split != j], k = i)
    #knn.pred <- knnMCN(train=train_cv, test=test_cv, TstX = NULL, K = i, ShowObs = F) 
    #TENIAMO PRESENTE CHE FACENDO NORMALIZE IN QUESTO MODO IL     RUSLTATO CAMBIA TOTALMENTE E PASSA DA k=1 a k=4 CHE HA SENSO??? O FORSE CAMBIO DRASTICO...NORMALIZZARE COME AVEVO FATTO PRIMA       ERA SBAGLIATO E QUESTO MIGLIORAMENTO LO ATTESTA (CON E SENZA NORMALIZZAZIONE C'E UN CAMBIAMENTO BELLO STRONG). RAZIONALE CHE        USCISSE CON k=1 CHE QUINDI PROBABILMENTE ERA UNA E UNA SOLA VARIABILE DELL'UNICO PUNTO VICINO A FARE LA DIFFERENZA
    mat.2[i,j+2]<-mean(knn.pred == labels.train[split ==j])
  }
    
  
  train.cv.tree <- transformed.glass.train[split != j,] #ho provato con quelli normalizzati ma non cambia nulla
  test.cv.tree  <-transformed.glass.train[split == j,]
  tree.glass.cv <- tree(Type ~. , train.cv.tree)
  #metodo alberi
  for (l in 1:tree.size.max){
    pruned.tree <-prune.tree(tree.glass.cv ,method = "misclass", k = pruned.tree.k[l+1] )
    tree.pred <-predict(pruned.tree , test.cv.tree[,-k.train] , type = "class")
    mat.2[knn.kmax+l,j+2]<- mean(tree.pred == labels.train[split ==j])
  }
  train.maha <- transformed.glass.train[split != j,] 
  test.maha <-transformed.glass.train[split ==j,]
  #metodo knn maha
  for (h in 1:knn.mahalanobis.max){
    mat.2[knn.kmax+tree.size.max+h,j+2]<- knn_mahalanobis(train.maha[,-k.train], test.maha[,-k.train], train.maha$Type, test.maha$Type, k=h)
  }
  
}
res.2<-cbind(mat.2[,c(1,2)],apply(mat.2[,3:j+2],MARGIN=1,mean)) 
colnames(res.2)<-c("metodo","hyperparameter","accuracy")
res.2<-as_tibble(res.2)
best.res.2<-rbind(res.2%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
                  res.2%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)),
                  res.2%>%filter(metodo=="knn.maha")%>%filter(accuracy==max(accuracy)))
best.res.2

```


```{r grafico confronto transformed variables}
res.2 %>%
  ggplot(mapping  = aes(x = hyperparameter, y= accuracy, color=metodo))  +
  geom_line()+ 
  geom_point()+
  facet_grid(~metodo) +
  labs(title= "Variazione di Accuracy Rispetto a Iperparametri") #DA CAMBIARE IL TITOLO....
 
```

# Test e conclusioni



```{r test knn variabili trasformate}
k<-7
out.norm<- normalize(transformed.glass.train[,-k],transformed.glass.test[,-k]) 
train.norm<-out.norm$train_norm
test.norm<-out.norm$test_norm
labels.train<-transformed.glass.train$Type

set.seed(47)
knn.pred.transf <- knn(train=train.norm, test=test.norm, cl=labels.train, k = 3) #non funziona se ci escono 2 o più soluzioni
#TOCCA SCEGLIERE IL SEED E USIAMO QUEL CHE CI ESCE
mean(transformed.glass.test$Type == knn.pred.transf) #molto più alto rispetto al validation...temo che usare solo 214 record è poco...
```



```{r Confusion Matrix}
reference<-transformed.glass.test$Type #VEDETE SE INSERIRE, PER ME CI STA
pred.best<-knn.pred.transf
confusionMatrix(data = pred.best,reference = reference, dnn = c("predicted values","true values")) #scegliete se value o values...
```


