---
title: "Progetto "
author: "Simone Gervasoni Angelo Cardinale Riccardo Bonamoni "
date: "2024-04-05"
output:
  html_document:
    self_contained : TRUE
    toc : TRUE
    toc_depth: 2 
    number_sections: yes
    toc_float: 
      collapsed: no
      smooth_scroll: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
knitr::opts_chunk$set(comment = NA)
```

```{r librerie, warning=FALSE, message= FALSE }
library(tidyverse)
library(mclust)
library(Rmixmod)
library(GGally)
library(caret)
library("tree")
library(ggcorrplot)
library(class)
library(nnet)
#library(knnGarden)
```

# Abstract

In questo progetto viene analizzato un dataset contenente diverse tipologie di vetri.
L'obiettivo è la classificazione di diversi vetri in studio, tramite l'analisi del loro indice di rifrazione e di alcuni elementi chimici presenti.
Nel lavoro verranno utilizzate e confrontate (TOT) tecniche di classificazione ovvero (…).


# Introduzione

Il vetro è un materiale solido che viene detto amorfo, in quanto si comporta come un solido a temperatura ambiente, ma possedendo a livello microscopico una struttura disordinata e rigida, al contrario della struttura ordinata dei normali solidi. Il vetro deriva infatti dalla solidificazione di un liquido.
I normali vetri sono prevalentemente costituiti da ossido di silicio, ma spesso in fase di produzione vengono aggiunte altre sostanze che ne vanno a modificare le proprietà e le funzioni. 

La previsione della tipologia di certi vetri può essere rilevante in alcuni ambiti. 
I dati utilizzati in questo progetto erano infatti stati raccolti con la finalità di indagini di criminologia. Tramite lo studio della classificazione del tipo di vetro è infatti possibile ottenere diverse evidenze. Ad esempio, un bicchiere sulla scena di un crimine può essere usato come una prova.
Altre ricerche, della University of California, sono volte, tramite la classificazione, a determinare se un determinato vetro sia float o meno.
Il vetro float, o vetro galleggiante, è infatti ad oggi il vetro più prodotto e utilizzato sul mercato. Grazie alle sue qualità è infatti ideale per diverse applicazioni come vetri per automobili, specchi, finestre o porte, ma anche per la produzione di vetri più specializzati come quello temperato, smerigliato, laminato o insonorizzato.

Il fine del progetto è quindi quello di trovare modelli che classifichino, con maggiore precisione possibile, la tipologia di ciascun vetro, data la sua composizione e analisi chimica.

QUI POTREMO DIRE BREVEMENTE I METODI UTILIZZATI E SOPRATTUTO QUELLI NON USATIE PERCHè


# Materiali

Il dataset impiegato, *Glass Classification* è stato scaricato dalla piattaforma Kaggle. Proviene dalla USA Forensic Science Service ed è messo a disposizione anche dalla University of California, le quali lo hanno utilizzato per le indagini e analisi di cui sopra.

La matrice dei dati è composta da 214 osservazioni e 10 variabili, riportate di seguito.

*RI: l'indice di rifrazione del vetro, una grandezza adimensionale
*8 elementi chimici, tutti espressi in percentuale in peso all'interno dell'ossido corrispondente.

  * Na: Sodio
  * Mg: Magnesio
  * Al: Alluminio
  * Si: Silicio
  * K: Potassio
  * Ca: Calcio
  * Ba: Bario
  * Fe: Ferro
  
*Type : La tipologia di vetro, è la __variabile target__. È suddivisa in 7 classi.

  1. building_windows_float_processed
  2. building_windows_non_float_processed
  3. vehicle_windows_float_processed
  4. vehicle_windows_non_float_processed 
  5. containers
  6. tableware
  7. headlamps

La variabile target è analizzata come un *Factor* in R. Per la classe 4 non sono presenti valori nel dataset.

Sono state effettuate analisi per il controllo di valori mancanti ed è stato riscontrato che non sono presenti rendendo quindi non necessarie tutte le tecniche volte all'imputazione o rimozione di essi.




# Analisi esplorativa

# Preprocessing
```{r import e inizio }
path <- "glass.csv"
glass <- tibble(read.table(path , sep = "," , header = T))
glass$Type <- factor(glass$Type)
n<-nrow(glass)
k<-ncol(glass)
```

In questa fase di preprocessing, come primo step, è stato diviso il dataset in training e test. Il training rappresenta un 85% casuale dei nostri dati, mentre il test il restante 15%. Le analisi, sia descrittive che grafiche, svolte successivamente sono applicate solamente al dataset di training, escludendo momentaneamente il test set.

```{r division dataset}
#questa è una funzione che divide tra test e train (magari ci serve nel CV)
dataset_division <- function(D1, perc = 0.7) {
  set.seed(42)
  id <- sample(size = round(perc* nrow(D1)) , x = nrow(D1))
  train <- D1[id,]
  test <- D1[-id,]
  out <- list(train = train, test = test)
}

#Qua abbiamo diviso magari dobbiamo normalizzare 
set.seed(123)
glass.train <- dataset_division(glass,perc=0.85)$train #non ha senso 30% nel test set se facciamo cv (????)
glass.test <- dataset_division(glass,perc=0.85)$test 

#non trovo il comando per farlo per tutto il data set senza usare apply (dovremmo fare un apply)
# ma inoltre per ogni colonna (se normalizziamo) dobbiamo salvare min e max (e dopo veranno applicati sul test)

```

Innanzitutto, è rappresentata la matrice di correlazione tra tutte le nostre variabili, tutte quantitative, esclusa ovviamente la variabile target. Si nota una forte correlazione positiva tra Ca e RI.
((((( per alberi non fa niente ma per Knn andrà vista ))))

```{r ggcorplots}
# glass.train%>%
#   select(-Type) %>%
#   ggpairs(mapping = aes(color = glass.train$Type))  
ggcorrplot(cor(glass.train%>%
 select(-Type)), lab = T , type = "lower" ) +
  labs(title = "Plot di Correlazioni Lineari")
```
Successivamente sono visualizzati i due istogrammi relativi alle frequenze, rispettivamente, realtive e assolute delle 7 classi della variabile target *Type*. Si ricorda che la quarta classe non è presente, riducendo a 6 classi la nostra analisi.

((((probabilmente c'è da trattare problema di dati sbilanciati con smote per 3 5 e 6 
dobbiamo scegliere quale tra le due prendere )))))

```{r barplot}
#questo bar plot è riguardo freq relative
glass.train %>%
  ggplot(aes(x= Type, y = after_stat(count)/sum(after_stat(count)), fill = Type)) + 
  geom_bar() +
  labs(title = "Frequenza Relativa delle Classi",y= "Freq Rel")
 
#questo bar plot è riguardo freq assolute
glass.train %>%
  ggplot(aes(x= Type, y = after_stat(count), fill = Type)) + 
  geom_bar() +
  labs(title = "Frequenza Assoluta delle Classi",y= "Freq Ass")
```


```{r}
glass.train %>% 
  select(-Type)%>%
  gather(key = tipo, value=valore)%>%
  ggplot(mapping= aes(y = valore)) +
  geom_boxplot()+
  facet_wrap(~ tipo,scales="free")+
  labs(title = "Box Plot con Tutte le Variabili")
  
```




```{r}
#qua devo ancora capire come farlo meglio sarebbero i box plot condizionati e ad ogni misurazione ad ogni tipo il problema è la scala di gradezza è diversa per ogni casella
glass.train %>%
  gather(key = Measure, value = valore, -Type )  %>%
  ggplot(aes(x = Type, y= valore, color = Measure)) +
  geom_violin(trim = F)+ 
  geom_boxplot(width=0.1) + 
  facet_wrap( ~ Measure ,scales = "free", ncol =3)+
  labs(title = "Vilion plot condizionato alle classi") 
#TODO boxplot confronto Ca e RI x la scelta di quella da togliere in knn
#Ma perchè metti To do qua quando c'è il read me (?), io ti consiglio di applicare malanobis
#che tiene conto di corr lineari
```
Questo grafico ci riesce a far capire 3 cose : 
1) la varianza e la scale dei vari materiali è diversa tra loro  ( ci porta a fare normalizzazione per knn)
2) la varianza tra diversi type per uno stesso elemento chimico è diversa ( LDA rifiutata )
3) la distribuzione condizionata per ogni elemento chimico (non è quasi mai normale) (Analisi discriminante rifiutata (LDA e QDA)) è estremamente assimetrica 

vediamo inoltre che per type = 6 BA FE K non variano assolutamente ( magari fare un altro grafico) 
```{r}
glass.train %>%
  filter(Type == "6")
```
```{r}
glass.train %>%
  filter(if_any("RI" : "Fe", ~ . > 0 ))
```

ulla
```{r}
glass.train %>%
  gather(key = Measure, value = valore, -Type )  %>%
  ggplot(aes(x= valore , color = Type)) +
  geom_density() +
  facet_wrap(~Measure, scales= "free")
```

```{r}
library(car)
p <-glass.train %>% 
  select( -c("Fe", "K" , "Ba", "Type"))
p.test <-glass.test %>%
  select( -c("Fe", "K" , "Ba", "Type"))
p1 <- p %>% 
  apply(MARGIN =2 , FUN = powerTransform, family = "bcnPower")  
a3 <- as.vector(c( p1$RI[1]$lambda,p1$Na[1]$lambda , p1$Mg[1]$lambda, p1$Al[1]$lambda, p1$Si[1]$lambda,p1$Ca[1]$lambda))
colnames(p)
col
transformedY <- bcPower(with(p, cbind(RI, Na, Mg, Al, Si, Ca)),
                lambda = a3 ,gamma= 0.01)
transformedY.test <- bcPower(with(p.test, cbind(RI, Na, Mg, Al, Si, Ca)),
                lambda = a3 ,gamma= 0.01)

transformed.glass.train <- tibble(cbind(as.data.frame(transformedY)), Type = glass.train$Type)
transformed.glass.test <- tibble(cbind(as.data.frame(transformedY.test)), Type = glass.test$Type)

transformed.glass.train%>%
  gather(key = Measure, value = valore, -Type )  %>%
  ggplot(aes(x= valore , colour= Type)) +
  geom_density() +
  facet_wrap(~Measure, scales= "free")

```


neanche niente male per alcune di queste

```{r shapiro test}
#qua si crea tabella pvalues
pvalue_shapiro <- matrix(0, nrow = (dim(glass.train)[2]-1), ncol = 6)
rownames(pvalue_shapiro) = colnames(glass.train)[-10]
colnames(pvalue_shapiro) = levels(glass.train$Type)
#-----

# Test Shapiro e costruzione di una matrice riassuntiva con i p-value condizionati alla classe
for (i in colnames(glass.train)[-10]){
  for (j in levels(glass.train$Type)){
    pvalue_shapiro[i,j]<-
      shapiro.test( jitter(glass.train %>% filter(Type == j)%>%pull(i)) )$p.value #qua si assegnano tutti i valori, bisonga stare attenti perchè alcuni di questi non sono validi perchè ho messo jitter non lo fa andare in errore quando tutti i valori sono uguali ma introduce noise gaussiano che rende normali quando non sono ugugali
    # pull invece serve per tirare fuori dal tibble un vettore (e non un tibble)
  }
}

pvalue_shapiro["Ca", "6" ] <- pvalue_shapiro["K", "6" ] <- pvalue_shapiro["Fe", "6" ] <- NA

round(pvalue_shapiro, 5)
```

```{r shapiro test dati trasformati}
#qua si crea tabella pvalues
pvalue_shapiro_2 <- matrix(0, nrow = (dim(transformed.glass.train)[2]-1), ncol = 6)
rownames(pvalue_shapiro_2) = colnames(transformed.glass.train)[-7]
colnames(pvalue_shapiro_2) = levels(transformed.glass.train$Type)
#-----

# Test Shapiro e costruzione di una matrice riassuntiva con i p-value condizionati alla classe
for (i in colnames(transformed.glass.train)[-7]){
  for (j in levels(glass.train$Type)){
    pvalue_shapiro_2[i,j]<-
      shapiro.test( jitter(transformed.glass.train %>% filter(Type == j)%>%pull(i)) )$p.value #qua si assegnano tutti i valori, bisonga stare attenti perchè alcuni di questi non sono validi perchè ho messo jitter non lo fa andare in errore quando tutti i valori sono uguali ma introduce noise gaussiano che rende normali quando non sono ugugali
    # pull invece serve per tirare fuori dal tibble un vettore (e non un tibble)
  }
}

#pvalue_shapiro["Ca", "6" ] <- pvalue_shapiro["K", "6" ] <- pvalue_shapiro["Fe", "6" ] <- NA

round(pvalue_shapiro_2, 5)
```




# Metodi di classificazione




## Alberi
```{r alberi}
#qua ho fatto un albero e sembra buono (migliorerà soltanto se normalizziam
set.seed(123)

tree.glass <- tree(Type ~. , glass.train)
summary(tree.glass)
tree.pred <- predict(tree.glass , glass.test[-10] , type = "class")
plot(tree.glass); text(tree.glass, , pretty = 0) # albero completo # queste due linee vanno eseguite insieme
#mean(tree.pred == glass.test$Type) 
pruned.tree.k <-prune.tree(tree.glass  ,method="misclass")$k
```
vediamo split inutili 

vediamo che con 11 leaf node è quello che predice meglio

5% improvement , but we can see that we have very little data for example class 6 has only 1 data point in test







```{r}
#v è un vettore colonna il primo valore è il minimo il secondo il massimo e il resto solo i valori effettivi delle colonne (Fe, Ba, etc)

#funzione che prende vettore, toglie il primo numero, e lo divide da secondo - primo 


func<-function(v){
  return((v-v[1])/(v[2]-v[1])) # non aggiorno v[1] v[2] fino alla fine e quelli saranno innutili
}
prova <-c(1,2,rnorm(100))
func(prova) #vediamo v[1] -> 0 v[2]->1

# matrix sarà la nostra tabella min e max
norm_aux<-function(data,matrix){
  data_2<-rbind(matrix,data)  
  data_norm<-apply(data_2,MARGIN = 2,func) #applichiamo func ad ogni colonna func
  return(data_norm[3:nrow(data_norm),]) # da 3 in poi perchè i primi due valori sono inutili 
}

# questa funzione prende sia il train che il test e normalizza il test con min e max del train
normalize_2<-function(train,test){
  matrix_min_max<-rbind(apply(train,MARGIN = 2,min),apply(train,MARGIN = 2,max)) #qua salviamo min e max -> matrice che anda in norm aux
  train_norm<-norm_aux(train,matrix_min_max)
  test_norm<-norm_aux(test,matrix_min_max)
  return(list(train_norm=train_norm,test_norm=test_norm))
} 

out<-normalize_2(glass.train[,-ncol(glass.train)],glass.test[,-ncol(glass.test)])
# out$train_norm[1:10,]
# out$test_norm[1:10,]
#mi viene da piangere, ha runnato tutto senza bug
#gervi fai un check che poi possiamo usarla
```
## KNN 





```{r knn 1}
#esiste anche knn.cv !!!!!!!!!
Kmax<-20
knn.pred.k<-rep(0,Kmax)
X_train<- out$train_norm
labels<-glass.train$Type #qua ho cambiato e non sembra essere cambiato nulla
X_test<- out$test_norm
for (i in 1:Kmax){
  set.seed(42)
  knn.pred <- knn(train=X_train, test=X_test, cl=labels, k = i)
  knn.pred.k[i]<-mean(knn.pred == glass.test$Type)
}
r<-rep("black",Kmax)
r[which.max(knn.pred.k)]<-"red"
plot(1:Kmax,knn.pred.k,type="h",ylim=c(0,1),col=r)
abline(h=max(knn.pred.k),col="purple") #mi sa che ho rotto qualcosa ma è un po' incoprensibile che cosa
```
#dealing with multicollinearity

```{r}


glass.train %>%
  select(Ca,RI,Type)%>%
  gather(key = Measure, value = valore, -Type )  %>%
  ggplot(aes(x = Type, y= valore, color = Measure)) +
  geom_violin(trim = F)+
  geom_boxplot(width=0.1) + 
  facet_wrap( ~ Measure ,scales = "free", ncol =3)+
  labs(title = "Vilion plot condizionato alle classi") #presumo Ca sia un po più impattante di RI però non porta 
  #ad alcun miglioramento toglierne una sola



```

```{r}
# 
# #si prova con mahalanobis distance
# 
# mahalanobis_dist <- function(x, mean_vec, sigma_inv) {
#   diff_vec <- t(t(x) - mean_vec)
#   out <- sqrt(rowSums((diff_vec %*% sigma_inv) * diff_vec))
#   return(out)
# }
# 
# find_mode <- function(x) {
#   u <- unique(x)
#   tab <- tabulate(match(x, u))
#   u[tab == max(tab)]
# }
# 
# predict_label <- function(v, train, mat, k, train_lab) {
#   dists <- apply(train, MARGIN = 1, function(x) mahalanobis_dist(x, v, mat))
#   check <- data.frame("dist"=dists,"lab"=train_lab)
#   check <- as.tibble(check)
#   check_ord <- top_n(check, k, wt = dist)
#   return(find_mode(as.factor(check_ord$lab)))
# }
# 
# # si implementa una funzione per l'applicazione del metodo knn tramite distanza di mahalanobis
# knn_mahalanobis <- function(train, valid, train_lab, valid_lab, k) {
#   x_matrix_inv <- solve(cor(train))
#   pred <- apply(valid, MARGIN = 1, function(x) predict_label(x, train, x_matrix_inv, k, train_lab))
#   out <- mean(as.factor(pred) == valid_lab)
#   return(out)
# }
# 
# 
# 
# knn_mahalanobis(glass.train[,1:9], glass.test[,1:9], glass.train$Type, glass.test$Type, k=5) #NON VA
```
questo chunk è anche buggato






##Confronto metodi


```{r}
set.seed(900) # teniamo questo
cv<-7
p<-1-1/cv
n.train<-nrow(glass.train)
k.train <-ncol(glass.train)
labels_train <-glass.train$Type
knn.kmax<-10
tree.size.max<- length(pruned.tree.k)-2
mat<-as.data.frame(matrix(NA,nrow=(knn.kmax+tree.size.max),ncol=cv+2))
mat[1:knn.kmax,1]<-"knn"
mat[(knn.kmax+1):(knn.kmax+tree.size.max),1]<-"tree"
mat[1:knn.kmax,2]<-1:knn.kmax
mat[(knn.kmax+1):(knn.kmax+tree.size.max),2]<- pruned.tree.k[2:(length(pruned.tree.k)-1)]

set.seed(123)
split<-sample(rep(c(1:cv),n.train/cv), size=n.train) #non ha senso questo metodo, capisco avere stessa proporzione ma usiamo una tecnica più flessibile se modifichiamo cv o proporzione train/test all'inizio (30% nel test set mi sembra eccessivo)

#loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv
for (j in 1:cv){
  
  
  out_norm_cv<-normalize_2(glass.train[split != j  ,-k.train] ,glass.train[split ==j ,-k.train])
  train_cv<-out_norm_cv$train_norm
  test_cv<-out_norm_cv$test_norm
  #metodo knn
  for (i in 1:knn.kmax){
    knn.pred <- knn(train=train_cv, test=test_cv, cl=labels_train[split != j], k = i)
    #knn.pred <- knnMCN(train=train_cv, test=test_cv, TstX = NULL, K = i, ShowObs = F) 
    #TENIAMO PRESENTE CHE FACENDO NORMALIZE IN QUESTO MODO IL     RUSLTATO CAMBIA TOTALMENTE E PASSA DA k=1 a k=4 CHE HA SENSO??? O FORSE CAMBIO DRASTICO...NORMALIZZARE COME AVEVO FATTO PRIMA       ERA SBAGLIATO E QUESTO MIGLIORAMENTO LO ATTESTA (CON E SENZA NORMALIZZAZIONE C'E UN CAMBIAMENTO BELLO STRONG). RAZIONALE CHE        USCISSE CON k=1 CHE QUINDI PROBABILMENTE ERA UNA E UNA SOLA VARIABILE DELL'UNICO PUNTO VICINO A FARE LA DIFFERENZA
    mat[i,j+2]<-mean(knn.pred == labels_train[split ==j])
  }
    
  
  train.cv.tree <- glass.train[split != j  ,-k.train] 
  test.cv.tree  <-glass.train[split ==j ,-k.train]
  train.cv.tree$Type <-labels_train[split != j]
  test.cv.tree$Type <-labels_train[split ==j]
  tree.glass.cv <- tree(Type ~. , train.cv.tree)
  #metodo alberi
  for (i in 1:tree.size.max){
    pruned.tree <-prune.tree(tree.glass.cv ,method="misclass", k= pruned.tree.k[i+1] )
    tree.pred <-predict(pruned.tree , test.cv.tree[-10] , type = "class")
    mat[knn.kmax+i,j+2]<- mean(tree.pred == labels_train[split ==j])
  }
}
res<-cbind(mat[,c(1,2)],apply(mat[,3:j+2],MARGIN=1,mean)) #possiamo usare median se vogliamo (meglio di no..)
colnames(res)<-c("metodo","hyperparameter","accuracy")
res<-as.tibble(res)
best.res<-rbind(res%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
      res%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)))
best.res
```

loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv



```{r grafico confronto}
res %>%
  ggplot(mapping  = aes(x = hyperparameter, y= accuracy, color=metodo))  +
  geom_line()+ 
  geom_point()+
  facet_grid(~metodo) +
  labs(title= "Variazione di Accuracy Rispetto a Iperparametri")
 
```

bisogna far capire che gli iperparametri qua non sono sovraponibili per 2 ragioni 
più si va a destra per il knn meno il modello è variabile (è il contrario per gli alberi)
non stanno misurando la stessa cosa

# Test e conclusioni

```{r test knn}
out_norm<- normalize_2(glass.train[,-k.train],glass.test[,-k.train]) #k è lo stesso in ogni caso dai
train_norm<-out_norm$train_norm
test_norm<-out_norm$test_norm


set.seed(123)
knn.pred <- knn(train=train_norm, test=test_norm, cl=labels_train, k = 1) #non funziona se ci escono 2 o più soluzioni
#TOCCA SCEGLIERE IL SEED E USIAMO QUEL CHE CI ESCE
mean(glass.test$Type == knn.pred) #molto più alto rispetto al validation...temo che usare solo 214 record è poco...
```


```{r test trees}
#senza normalizzare 

#qui seed non incide???
tree.best.glass <- tree(Type ~. , glass.train)
pruned.best.tree <-prune.tree(tree.best.glass  ,method="misclass", k = best.res %>% filter(metodo=="tree") %>% pull(hyperparameter) ) 
tree.best.pred <-predict(pruned.best.tree  , glass.test[-10] , type = "class")

mean(glass.test$Type == tree.best.pred) 
#BISOGNA RIVEDERE UN PO TUTA LA QUESTIONE cv E PERCENTUALE NEL TEST SET (O RIMETTIAMO cv=5 E perc=0.7 CHE COMUNQUE PER POCHI RECORD
#HA SENSO E CI VIENE UN OUTPUT CARINO...PERò CON COSì TANTI DATI NEL TEST SET TEMO SI SOVRASTIMI LA PRECISIONE (MINOR BIAS A FRONTE DI MAGGIOR VARIANZA CHE HA SENSO CAMBIANDO IL seed OTTENIAMO VALORI SBALLATI...) -> quel poco che sono riuscito a capire mi sembra sbagliato prova a mandare un audio 
```
## Logistic Multi-Class

```{r test logistic Multi-Class}
#mi sa che sta cosa me la sono inventata...
mod<-multinom(formula = Type~.,data = glass.train)
pred<-predict(mod,glass.test[,-k])
pred
mean(pred==glass.test$Type) #FENOMENALE (DA RIVEDERE)
```



## Transformed variables

```{r cv transformed variables}
set.seed(900) # teniamo questo
cv<-7
p<-1-1/cv
n.train<-nrow(transformed.glass.train)
k.train <-ncol(transformed.glass.train)
labels_train <-transformed.glass.train$Type
knn.kmax<-10
tree.size.max<- length(pruned.tree.k)-2
mat_2<-as.data.frame(matrix(NA,nrow=(knn.kmax+tree.size.max),ncol=cv+2))
mat_2[1:knn.kmax,1]<-"knn"
mat_2[(knn.kmax+1):(knn.kmax+tree.size.max),1]<-"tree"
mat_2[1:knn.kmax,2]<-1:knn.kmax
mat_2[(knn.kmax+1):(knn.kmax+tree.size.max),2]<- pruned.tree.k[2:(length(pruned.tree.k)-1)]

set.seed(123)
split<-sample(rep(c(1:cv),n.train/cv), size=n.train) #non ha senso questo metodo, capisco avere stessa proporzione ma usiamo una tecnica più flessibile se modifichiamo cv o proporzione train/test all'inizio (30% nel test set mi sembra eccessivo)

#loop più grande è quello del cv per ogni split del cv si allenano tutti i modelli con tutti gli iperparametri diversi si calcola l'errore e si passa al prossimo blocco di cv
for (j in 1:cv){
  
  out_norm_cv_2<-normalize_2(transformed.glass.train[split != j  ,-k.train] ,transformed.glass.train[split ==j ,-k.train])
  train_cv<-out_norm_cv_2$train_norm
  test_cv<-out_norm_cv_2$test_norm
  #metodo knn
  for (i in 1:knn.kmax){
    knn.pred <- knn(train=train_cv, test=test_cv, cl=labels_train[split != j], k = i)
    #knn.pred <- knnMCN(train=train_cv, test=test_cv, TstX = NULL, K = i, ShowObs = F) 
    #TENIAMO PRESENTE CHE FACENDO NORMALIZE IN QUESTO MODO IL     RUSLTATO CAMBIA TOTALMENTE E PASSA DA k=1 a k=4 CHE HA SENSO??? O FORSE CAMBIO DRASTICO...NORMALIZZARE COME AVEVO FATTO PRIMA       ERA SBAGLIATO E QUESTO MIGLIORAMENTO LO ATTESTA (CON E SENZA NORMALIZZAZIONE C'E UN CAMBIAMENTO BELLO STRONG). RAZIONALE CHE        USCISSE CON k=1 CHE QUINDI PROBABILMENTE ERA UNA E UNA SOLA VARIABILE DELL'UNICO PUNTO VICINO A FARE LA DIFFERENZA
    mat_2[i,j+2]<-mean(knn.pred == labels_train[split ==j])
  }
    
  
  train.cv.tree <- transformed.glass.train[split != j  ,-k.train] 
  test.cv.tree  <-transformed.glass.train[split ==j ,-k.train]
  train.cv.tree$Type <-labels_train[split != j]
  test.cv.tree$Type <-labels_train[split ==j]
  tree.glass.cv <- tree(Type ~. , train.cv.tree)
  #metodo alberi
  for (i in 1:tree.size.max){
    pruned.tree <-prune.tree(tree.glass.cv ,method="misclass", k= pruned.tree.k[i+1] )
    tree.pred <-predict(pruned.tree , test.cv.tree[-10] , type = "class")
    mat_2[knn.kmax+i,j+2]<- mean(tree.pred == labels_train[split ==j])
  }
}
res_2<-cbind(mat_2[,c(1,2)],apply(mat_2[,3:j+2],MARGIN=1,mean)) #possiamo usare median se vogliamo (meglio di no..)
colnames(res_2)<-c("metodo","hyperparameter","accuracy")
res<-as.tibble(res_2)
best.res_2<-rbind(res%>%filter(metodo=="knn")%>%filter(accuracy==max(accuracy)),
      res%>%filter(metodo=="tree")%>%filter(accuracy==max(accuracy)))
best.res_2

```


